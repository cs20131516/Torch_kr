{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch \ud504\ub85c\ud30c\uc77c\ub7ec(Profiler)\n====================================\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 \uc5b4\ub5bb\uac8c PyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\ub294\uc9c0, \uadf8\ub9ac\uace0 \ubaa8\ub378\uc758 \uc5f0\uc0b0\uc790\ub4e4\uc774 \uc18c\ube44\ud558\ub294 \uba54\ubaa8\ub9ac\uc640 \uc2dc\uac04\uc744 \uce21\uc815\ud558\ub294 \ubc29\ubc95\uc744 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uac1c\uc694\n------------\nPyTorch\ub294 \uc0ac\uc6a9\uc790\uac00 \ubaa8\ub378 \ub0b4\uc758 \uc5f0\uc0b0 \ube44\uc6a9\uc774 \ud070(expensive) \uc5f0\uc0b0\uc790\ub4e4\uc774 \ubb34\uc5c7\uc778\uc9c0 \uc54c\uace0\uc2f6\uc744 \ub54c \uc720\uc6a9\ud558\uac8c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub294 \uac04\ub2e8\ud55c \ud504\ub85c\ud30c\uc77c\ub7ec API\ub97c \ud3ec\ud568\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 \ubaa8\ub378\uc758 \uc131\ub2a5(performance)\uc744 \ubd84\uc11d\ud558\ub824\uace0 \ud560 \ub54c \uc5b4\ub5bb\uac8c \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud574\uc57c \ud558\ub294\uc9c0\ub97c \ubcf4\uc5ec\uc8fc\uae30 \uc704\ud574 \uac04\ub2e8\ud55c ResNet \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uc124\uc815(Setup)\n-------------\n``torch`` \uc640 ``torchvision`` \uc744 \uc124\uce58\ud558\uae30 \uc704\ud574\uc11c \uc544\ub798\uc758 \ucee4\ub9e8\ub4dc\ub97c \uc785\ub825\ud569\ub2c8\ub2e4:\n\n\n::\n\n   pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub2e8\uacc4(Steps)\n-------------\n\n1. \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4 \ubd88\ub7ec\uc624\uae30\n2. \uac04\ub2e8\ud55c ResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654 \ud558\uae30\n3. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud589\uc2dc\uac04 \ubd84\uc11d\ud558\uae30\n4. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba54\ubaa8\ub9ac \uc18c\ube44 \ubd84\uc11d\ud558\uae30\n5. \ucd94\uc801\uae30\ub2a5 \uc0ac\uc6a9\ud558\uae30\n\n1. \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ub4e4 \ubd88\ub7ec\uc624\uae30\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\uc774 \ub808\uc2dc\ud53c\uc5d0\uc11c\ub294 ``torch`` \uc640 ``torchvision.models``,\n\uadf8\ub9ac\uace0 ``profiler`` \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torchvision.models as models\nfrom torch.profiler import profile, record_function, ProfilerActivity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \uac04\ub2e8\ud55c ResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ud654 \ud558\uae30\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nResNet \ubaa8\ub378 \uc778\uc2a4\ud134\uc2a4\ub97c \ub9cc\ub4e4\uace0 \uc785\ub825\uac12\uc744\n\uc900\ube44\ud569\ub2c8\ub2e4 :\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud589\uc2dc\uac04 \ubd84\uc11d\ud558\uae30\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ucee8\ud14d\uc2a4\ud2b8 \uba54\ub2c8\uc800(context manager)\ub97c \ud1b5\ud574 \ud65c\uc131\ud654\ub418\uace0,\n\uc5ec\ub7ec \ub9e4\uac1c\ubcc0\uc218\ub97c \ubc1b\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc720\uc6a9\ud55c \uba87 \uac00\uc9c0 \ub9e4\uac1c\ubcc0\uc218\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:\n\n- ``activities`` - a list of activities to profile:\n   - ``ProfilerActivity.CPU`` - PyTorch operators, TorchScript functions and\n     user-defined code labels (see ``record_function`` below);\n   - ``ProfilerActivity.CUDA`` - on-device CUDA kernels;\n- ``record_shapes`` - \uc5f0\uc0ac\uc790 \uc785\ub825(input)\uc758 shape\uc744 \uae30\ub85d\ud560\uc9c0 \uc5ec\ubd80;\n- ``profile_memory`` - \ubaa8\ub378\uc758 \ud150\uc11c(Tensor)\ub4e4\uc774 \uc18c\ube44\ud558\ub294 \uba54\ubaa8\ub9ac \uc591\uc744 \ubcf4\uace0(report)\ud560\uc9c0 \uc5ec\ubd80;\n- ``use_cuda`` - CUDA \ucee4\ub110\uc758 \uc2e4\ud589\uc2dc\uac04\uc744 \uce21\uc815\ud560\uc9c0 \uc5ec\ubd80;\n\nNote: when using CUDA, profiler also shows the runtime CUDA events\noccuring on the host.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5b4\ub5bb\uac8c \uc2e4\ud589\uc2dc\uac04\uc744 \ubd84\uc11d\ud558\ub294\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``record_function`` \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc784\uc758\uc758 \ucf54\ub4dc \ubc94\uc704\uc5d0\n\uc0ac\uc6a9\uc790\uac00 \uc9c0\uc815\ud55c \uc774\ub984\uc73c\ub85c \ub808\uc774\ube14(label)\uc744 \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n(\uc704 \uc608\uc81c\uc5d0\uc11c\ub294 ``model_inference`` \ub97c \ub808\uc774\ube14\ub85c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.)\n\n\ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uba74 \ud504\ub85c\ud30c\uc77c\ub7ec \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub85c \uac10\uc2f8\uc9c4(wrap) \ucf54\ub4dc \ubc94\uc704\ub97c\n\uc2e4\ud589\ud558\ub294 \ub3d9\uc548 \uc5b4\ub5a4 \uc5f0\uc0b0\uc790\ub4e4\uc774 \ud638\ucd9c\ub418\uc5c8\ub294\uc9c0 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ub9cc\uc57d \uc5ec\ub7ec \ud504\ub85c\ud30c\uc77c\ub7ec\uc758 \ubc94\uc704\uac00 \ub3d9\uc2dc\uc5d0 \ud65c\uc131\ud654\ub41c \uacbd\uc6b0(\uc608. PyTorch \uc4f0\ub808\ub4dc\uac00 \ubcd1\ub82c\ub85c\n\uc2e4\ud589 \uc911\uc778 \uacbd\uc6b0), \uac01 \ud504\ub85c\ud30c\uc77c\ub9c1 \ucee8\ud14d\uc2a4\ud2b8 \uad00\ub9ac\uc790\ub294 \uac01\uac01\uc758 \ubc94\uc704 \ub0b4\uc758 \uc5f0\uc0b0\uc790\ub4e4\ub9cc\n\ucd94\uc801(track)\ud569\ub2c8\ub2e4.\n\ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ub610\ud55c ``torch.jit._fork`` \ub85c \uc2e4\ud589\ub41c \ube44\ub3d9\uae30 \uc791\uc5c5\uacfc\n(\uc5ed\uc804\ud30c \ub2e8\uacc4\uc758 \uacbd\uc6b0) ``backward()`` \uc758 \ud638\ucd9c\ub85c \uc2e4\ud589\ub41c \uc5ed\uc804\ud30c \uc5f0\uc0b0\uc790\ub4e4\ub3c4\n\uc790\ub3d9\uc73c\ub85c \ud504\ub85c\ud30c\uc77c\ub9c1\ud569\ub2c8\ub2e4.\n\n\uc704 \ucf54\ub4dc\ub97c \uc2e4\ud589\ud55c \ud1b5\uacc4\ub97c \ucd9c\ub825\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(\uba87\uba87 \uc5f4\uc744 \uc81c\uc678\ud558\uace0) \ucd9c\ub825\uac12\uc774 \uc774\ub807\uac8c \ubcf4\uc77c \uac83\uc785\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# ---------------------------------  ------------  ------------  ------------  ------------\n#                              Name      Self CPU     CPU total  CPU time avg    # of Calls\n# ---------------------------------  ------------  ------------  ------------  ------------\n#                   model_inference       5.509ms      57.503ms      57.503ms             1\n#                      aten::conv2d     231.000us      31.931ms       1.597ms            20\n#                 aten::convolution     250.000us      31.700ms       1.585ms            20\n#                aten::_convolution     336.000us      31.450ms       1.573ms            20\n#          aten::mkldnn_convolution      30.838ms      31.114ms       1.556ms            20\n#                  aten::batch_norm     211.000us      14.693ms     734.650us            20\n#      aten::_batch_norm_impl_index     319.000us      14.482ms     724.100us            20\n#           aten::native_batch_norm       9.229ms      14.109ms     705.450us            20\n#                        aten::mean     332.000us       2.631ms     125.286us            21\n#                      aten::select       1.668ms       2.292ms       8.988us           255\n# ---------------------------------  ------------  ------------  ------------  ------------\n# Self CPU time total: 57.549ms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc608\uc0c1\ud588\ub358 \ub300\ub85c, \ub300\ubd80\ubd84\uc758 \uc2dc\uac04\uc774 \ud569\uc131\uacf1(convolution) \uc5f0\uc0b0(\ud2b9\ud788 MKL-DNN\uc744 \uc9c0\uc6d0\ud558\ub3c4\ub85d\n\ucef4\ud30c\uc77c\ub41c PyTorch\uc758 \uacbd\uc6b0\uc5d0\ub294 ``mkldnn_convolution`` )\uc5d0\uc11c \uc18c\uc694\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n(\uacb0\uacfc \uc5f4\ub4e4 \uc911) Self CPU time\uacfc CPU time\uc758 \ucc28\uc774\uc5d0 \uc720\uc758\ud574\uc57c \ud569\ub2c8\ub2e4 -\n\uc5f0\uc0b0\uc790\ub294 \ub2e4\ub978 \uc5f0\uc0b0\uc790\ub4e4\uc744 \ud638\ucd9c\ud560 \uc218 \uc788\uc73c\uba70, Self CPU time\uc5d0\ub294 \ud558\uc704(child) \uc5f0\uc0b0\uc790 \ud638\ucd9c\uc5d0\uc11c \ubc1c\uc0dd\ud55c\n\uc2dc\uac04\uc744 \uc81c\uc678\ud574\uc11c, Totacl CPU time\uc5d0\ub294 \ud3ec\ud568\ud574\uc11c \ud45c\uc2dc\ud569\ub2c8\ub2e4.\nYou can choose to sort by the self cpu time by passing\n``sort_by=\"self_cpu_time_total\"`` into the ``table`` call.\n\n\ubcf4\ub2e4 \uc138\ubd80\uc801\uc778 \uacb0\uacfc \uc815\ubcf4 \ubc0f \uc5f0\uc0b0\uc790\uc758 \uc785\ub825 shape\uc744 \ud568\uaed8 \ubcf4\ub824\uba74 ``group_by_input_shape=True`` \ub97c\n\uc778\uc790\ub85c \uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4\n(note: this requires running the profiler with ``record_shapes=True``):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(prof.key_averages(group_by_input_shape=True).table(sort_by=\"cpu_time_total\", row_limit=10))\n\n# (\uba87\uba87 \uc5f4\uc740 \uc81c\uc678\ud558\uc600\uc2b5\ub2c8\ub2e4)\n# ---------------------------------  ------------  -------------------------------------------\n#                              Name     CPU total                                 Input Shapes\n# ---------------------------------  ------------  -------------------------------------------\n#                   model_inference      57.503ms                                           []\n#                      aten::conv2d       8.008ms      [5,64,56,56], [64,64,3,3], [], ..., []]\n#                 aten::convolution       7.956ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                aten::_convolution       7.909ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#          aten::mkldnn_convolution       7.834ms     [[5,64,56,56], [64,64,3,3], [], ..., []]\n#                      aten::conv2d       6.332ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                 aten::convolution       6.303ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                aten::_convolution       6.273ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#          aten::mkldnn_convolution       6.233ms    [[5,512,7,7], [512,512,3,3], [], ..., []]\n#                      aten::conv2d       4.751ms  [[5,256,14,14], [256,256,3,3], [], ..., []]\n# ---------------------------------  ------------  -------------------------------------------\n# Self CPU time total: 57.549ms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the occurence of ``aten::convolution`` twice with different input shapes.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Profiler can also be used to analyze performance of models executed on GPUs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18().cuda()\ninputs = torch.randn(5, 3, 224, 224).cuda()\n\nwith profile(activities=[\n        ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n    with record_function(\"model_inference\"):\n        model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Note: the first use of CUDA profiling may bring an extra overhead.)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting table output:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# (omitting some columns)\n# -------------------------------------------------------  ------------  ------------\n#                                                    Name     Self CUDA    CUDA total\n# -------------------------------------------------------  ------------  ------------\n#                                         model_inference       0.000us      11.666ms\n#                                            aten::conv2d       0.000us      10.484ms\n#                                       aten::convolution       0.000us      10.484ms\n#                                      aten::_convolution       0.000us      10.484ms\n#                              aten::_convolution_nogroup       0.000us      10.484ms\n#                                       aten::thnn_conv2d       0.000us      10.484ms\n#                               aten::thnn_conv2d_forward      10.484ms      10.484ms\n# void at::native::im2col_kernel<float>(long, float co...       3.844ms       3.844ms\n#                                       sgemm_32x32x32_NN       3.206ms       3.206ms\n#                                   sgemm_32x32x32_NN_vec       3.093ms       3.093ms\n# -------------------------------------------------------  ------------  ------------\n# Self CPU time total: 23.015ms\n# Self CUDA time total: 11.666ms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the occurence of on-device kernels in the output (e.g. ``sgemm_32x32x32_NN``).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \ud504\ub85c\ud30c\uc77c\ub7ec\ub97c \uc0ac\uc6a9\ud558\uc5ec \uba54\ubaa8\ub9ac \uc18c\ube44 \ubd84\uc11d\ud558\uae30\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPyTorch \ud504\ub85c\ud30c\uc77c\ub7ec\ub294 \ubaa8\ub378\uc758 \uc5f0\uc0b0\uc790\ub4e4\uc744 \uc2e4\ud589\ud558\uba70 (\ubaa8\ub378\uc758 \ud150\uc11c\ub4e4\uc774 \uc0ac\uc6a9\ud558\uba70) \ud560\ub2f9(\ub610\ub294 \ud574\uc81c)\ud55c\n\uba54\ubaa8\ub9ac\uc758 \uc591\ub3c4 \ud45c\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc544\ub798 \ucd9c\ub825 \uacb0\uacfc\uc5d0\uc11c 'Self' memory\ub294 \ud574\ub2f9 \uc5f0\uc0b0\uc790\uc5d0 \uc758\ud574 \ud638\ucd9c\ub41c \ud558\uc704(child) \uc5f0\uc0b0\uc790\ub4e4\uc744 \uc81c\uc678\ud55c,\n\uc5f0\uc0b0\uc790 \uc790\uccb4\uc5d0 \ud560\ub2f9(\ud574\uc81c)\ub41c \uba54\ubaa8\ub9ac\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4.\n\uba54\ubaa8\ub9ac \ud504\ub85c\ud30c\uc77c\ub9c1 \uae30\ub2a5\uc744 \ud65c\uc131\ud654\ud558\ub824\uba74 ``profile_memory=True`` \ub97c \uc778\uc790\ub85c \uc804\ub2ec\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18()\ninputs = torch.randn(5, 3, 224, 224)\n\nwith profile(activities=[ProfilerActivity.CPU],\n        profile_memory=True, record_shapes=True) as prof:\n    model(inputs)\n\nprint(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))\n\n# (\uba87\uba87 \uc5f4\uc740 \uc81c\uc678\ud558\uc600\uc2b5\ub2c8\ub2e4)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n#                       aten::addmm      19.53 Kb      19.53 Kb             1\n#               aten::empty_strided         572 b         572 b            25\n#                     aten::resize_         240 b         240 b             6\n#                         aten::abs         480 b         240 b             4\n#                         aten::add         160 b         160 b            20\n#               aten::masked_select         120 b         112 b             1\n#                          aten::ne         122 b          53 b             6\n#                          aten::eq          60 b          30 b             2\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms\n\nprint(prof.key_averages().table(sort_by=\"cpu_memory_usage\", row_limit=10))\n\n# (\uba87\uba87 \uc5f4\uc740 \uc81c\uc678\ud558\uc600\uc2b5\ub2c8\ub2e4)\n# ---------------------------------  ------------  ------------  ------------\n#                              Name       CPU Mem  Self CPU Mem    # of Calls\n# ---------------------------------  ------------  ------------  ------------\n#                       aten::empty      94.79 Mb      94.79 Mb           121\n#                  aten::batch_norm      47.41 Mb           0 b            20\n#      aten::_batch_norm_impl_index      47.41 Mb           0 b            20\n#           aten::native_batch_norm      47.41 Mb           0 b            20\n#                      aten::conv2d      47.37 Mb           0 b            20\n#                 aten::convolution      47.37 Mb           0 b            20\n#                aten::_convolution      47.37 Mb           0 b            20\n#          aten::mkldnn_convolution      47.37 Mb           0 b            20\n#                  aten::max_pool2d      11.48 Mb           0 b             1\n#     aten::max_pool2d_with_indices      11.48 Mb      11.48 Mb             1\n# ---------------------------------  ------------  ------------  ------------\n# Self CPU time total: 53.064ms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. \ucd94\uc801\uae30\ub2a5 \uc0ac\uc6a9\ud558\uae30\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ud504\ub85c\ud30c\uc77c\ub9c1 \uacb0\uacfc\ub294 .json \ud615\ud0dc\uc758 \ucd94\uc801 \ud30c\uc77c(trace file)\ub85c \ucd9c\ub825\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = models.resnet18().cuda()\ninputs = torch.randn(5, 3, 224, 224).cuda()\n\nwith profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:\n    model(inputs)\n\nprof.export_chrome_trace(\"trace.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc0ac\uc6a9\uc790\ub294 Chrome \ube0c\ub77c\uc6b0\uc800( ``chrome://tracing`` )\uc5d0\uc11c \ucd94\uc801 \ud30c\uc77c\uc744 \ubd88\ub7ec\uc640\n\ud504\ub85c\ud30c\uc77c\ub41c \uc77c\ub828\uc758 \uc5f0\uc0b0\uc790\ub4e4\uacfc CUDA \ucee4\ub110\uc744 \uac80\ud1a0\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n![](../../_static/img/trace_img.png)\n\n   :scale: 25 %\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Examining stack traces\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nProfiler can be used to analyze Python and TorchScript stack traces:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    with_stack=True,\n) as prof:\n    model(inputs)\n\n# Print aggregated stats\nprint(prof.key_averages(group_by_stack_n=5).table(sort_by=\"self_cuda_time_total\", row_limit=2))\n\n# (omitting some columns)\n# -------------------------  -----------------------------------------------------------\n#                      Name  Source Location\n# -------------------------  -----------------------------------------------------------\n# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n#                            .../torch/nn/modules/conv.py(443): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#                            .../site-packages/torchvision/models/resnet.py(63): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#\n# aten::thnn_conv2d_forward  .../torch/nn/modules/conv.py(439): _conv_forward\n#                            .../torch/nn/modules/conv.py(443): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#                            .../site-packages/torchvision/models/resnet.py(59): forward\n#                            .../torch/nn/modules/module.py(1051): _call_impl\n#\n# -------------------------  -----------------------------------------------------------\n# Self CPU time total: 34.016ms\n# Self CUDA time total: 11.659ms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the two convolutions and the two callsites in ``torchvision/models/resnet.py`` script.\n\n(Warning: stack tracing adds an extra profiling overhead.)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Visualizing data as a flamegraph\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nExecution time (``self_cpu_time_total`` and ``self_cuda_time_total`` metrics) and stack traces\ncan also be visualized as a flame graph. To do this, first export the raw data using ``export_stacks`` (requires ``with_stack=True``):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We recommend using e.g. `Flamegraph tool <https://github.com/brendangregg/FlameGraph>`_ to generate an\ninteractive SVG:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# git clone https://github.com/brendangregg/FlameGraph\n# cd FlameGraph\n# ./flamegraph.pl --title \"CUDA time\" --countname \"us.\" /tmp/profiler_stacks.txt > perf_viz.svg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](../../_static/img/perf_viz.png)\n\n   :scale: 25 %\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "8. Using profiler to analyze long-running jobs\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nPyTorch profiler offers an additional API to handle long-running jobs\n(such as training loops). Tracing all of the execution can be\nslow and result in very large trace files. To avoid this, use optional\narguments:\n\n- ``schedule`` - specifies a function that takes an integer argument (step number)\n  as an input and returns an action for the profiler, the best way to use this parameter\n  is to use ``torch.profiler.schedule`` helper function that can generate a schedule for you;\n- ``on_trace_ready`` - specifies a function that takes a reference to the profiler as\n  an input and is called by the profiler each time the new trace is ready.\n\nTo illustrate how the API works, let's first consider the following example with\n``torch.profiler.schedule`` helper function:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from torch.profiler import schedule\n\nmy_schedule = schedule(\n    skip_first=10,\n    wait=5,\n    warmup=1,\n    active=3,\n    repeat=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Profiler assumes that the long-running job is composed of steps, numbered\nstarting from zero. The example above defines the following sequence of actions\nfor the profiler:\n\n1. Parameter ``skip_first`` tells profiler that it should ignore the first 10 steps\n   (default value of ``skip_first`` is zero);\n2. After the first ``skip_first`` steps, profiler starts executing profiler cycles;\n3. Each cycle consists of three phases:\n\n   - idling (``wait=5`` steps), during this phase profiler is not active;\n   - warming up (``warmup=1`` steps), during this phase profiler starts tracing, but\n     the results are discarded; this phase is used to discard the samples obtained by\n     the profiler at the beginning of the trace since they are usually skewed by an extra\n     overhead;\n   - active tracing (``active=3`` steps), during this phase profiler traces and records data;\n4. An optional ``repeat`` parameter specifies an upper bound on the number of cycles.\n   By default (zero value), profiler will execute cycles as long as the job runs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Thus, in the example above, profiler will skip the first 15 steps, spend the next step on the warm up,\nactively record the next 3 steps, skip another 5 steps, spend the next step on the warm up, actively\nrecord another 3 steps. Since the ``repeat=2`` parameter value is specified, the profiler will stop\nthe recording after the first two cycles.\n\nAt the end of each cycle profiler calls the specified ``on_trace_ready`` function and passes itself as\nan argument. This function is used to process the new trace - either by obtaining the table output or\nby saving the output on disk as a trace file.\n\nTo send the signal to the profiler that the next step has started, call ``prof.step()`` function.\nThe current profiler step is stored in ``prof.step_num``.\n\nThe following example shows how to use all of the concepts above:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def trace_handler(p):\n    output = p.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=10)\n    print(output)\n    p.export_chrome_trace(\"/tmp/trace_\" + str(p.step_num) + \".json\")\n\nwith profile(\n    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n) as p:\n    for idx in range(8):\n        model(inputs)\n        p.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub354 \uc54c\uc544\ubcf4\uae30\n-------------\n\n\ub2e4\uc74c \ub808\uc2dc\ud53c\uc640 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc77d\uc73c\uba70 \ud559\uc2b5\uc744 \uacc4\uc18d\ud574\ubcf4\uc138\uc694:\n\n- :doc:`/recipes/recipes/benchmark`\n- :doc:`/intermediate/tensorboard_profiler_tutorial` \ud29c\ud1a0\ub9ac\uc5bc\n- :doc:`/intermediate/tensorboard_tutorial` \ud29c\ud1a0\ub9ac\uc5bc\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}