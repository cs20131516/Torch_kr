{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\uacf5\uac04 \ubcc0\ud615 \ub124\ud2b8\uc6cc\ud06c(Spatial Transformer Networks) \ud29c\ud1a0\ub9ac\uc5bc\n=====================================\n**\uc800\uc790**: `Ghassen HAMROUNI <https://github.com/GHamrouni>`_\n**\ubc88\uc5ed**: `\ud669\uc131\uc218 <https://github.com/adonisues>`_ , `\uc815\uc2e0\uc720 <https://github.com/SSinyu>`_\n.. figure:: /_static/img/stn/FSeq.png\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \uacf5\uac04 \ubcc0\ud615 \ub124\ud2b8\uc6cc\ud06c(spatial transformer networks, \uc774\ud558 STN)\uc774\ub77c\n\ubd88\ub9ac\ub294 \ube44\uc8fc\uc5bc \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc744 \uc774\uc6a9\ud574 \uc2e0\uacbd\ub9dd\uc744 \uc99d\uac15(augment)\uc2dc\ud0a4\ub294 \ubc29\ubc95\uc5d0 \ub300\ud574\n\ud559\uc2b5\ud569\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 `DeepMind paper <https://arxiv.org/abs/1506.02025>`__ \uc5d0\uc11c\n\ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nSTN\uc740 \uc5b4\ub5a0\ud55c \uacf5\uac04\uc801 \ubcc0\ud615(spatial transformation)\uc5d0\ub3c4 \uc801\uc6a9\ud560 \uc218 \uc788\ub294 \ubbf8\ubd84 \uac00\ub2a5\ud55c\n\uc5b4\ud150\uc158\uc758 \uc77c\ubc18\ud654\uc785\ub2c8\ub2e4. \ub530\ub77c\uc11c STN\uc740 \uc2e0\uacbd\ub9dd\uc758 \uae30\ud558\ud559\uc801 \ubd88\ubcc0\uc131(geometric invariance)\uc744\n\uac15\ud654\ud558\uae30 \uc704\ud574 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ub300\uc0c1\uc73c\ub85c \uc5b4\ub5a0\ud55c \uacf5\uac04\uc801 \ubcc0\ud615\uc744 \uc218\ud589\ud574\uc57c \ud558\ub294\uc9c0 \ud559\uc2b5\ud558\ub3c4\ub85d\n\ud569\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uc5b4 \uc774\ubbf8\uc9c0\uc758 \uad00\uc2ec \uc601\uc5ed\uc744 \uc798\ub77c\ub0b4\uac70\ub098, \ud06c\uae30\ub97c \uc870\uc815\ud558\uac70\ub098, \ubc29\ud5a5(orientation)\uc744\n\uc218\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. CNN\uc740 \uc774\ub7ec\ud55c \ud68c\uc804, \ud06c\uae30 \uc870\uc815 \ub4f1\uc758 \uc77c\ubc18\uc801\uc778 \uc544\ud540(affine) \ubcc0\ud658\ub41c\n\uc785\ub825\uc5d0 \ub300\ud574 \uacb0\uacfc\uc758 \ubcc0\ub3d9\uc774 \ud06c\uae30 \ub54c\ubb38\uc5d0 (\ubbfc\uac10\ud558\uae30 \ub54c\ubb38\uc5d0), STN\uc740 \uc774\ub97c \uadf9\ubcf5\ud558\ub294\ub370 \ub9e4\uc6b0\n\uc720\uc6a9\ud55c \uba54\ucee4\ub2c8\uc998\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nSTN\uc774 \uac00\uc9c4 \uc7a5\uc810 \uc911 \ud558\ub098\ub294 \uc544\uc8fc \uc791\uc740 \uc218\uc815\ub9cc\uc73c\ub85c \uae30\uc874\uc5d0 \uc0ac\uc6a9\ud558\ub358 CNN\uc5d0 \uac04\ub2e8\ud558\uac8c \uc5f0\uacb0 \uc2dc\ud0ac\n\uc218 \uc788\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ub77c\uc774\uc13c\uc2a4: BSD\n# \uc800\uc790: Ghassen Hamrouni\n\nfrom __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.ion()   # \ub300\ud654\ud615 \ubaa8\ub4dc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\n----------------\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 MNIST \ub370\uc774\ud130\uc14b\uc744 \uc774\uc6a9\ud574 \uc2e4\ud5d8\ud569\ub2c8\ub2e4. \uc2e4\ud5d8\uc5d0\ub294 STN\uc73c\ub85c\n\uc99d\uac15\ub41c \uc77c\ubc18\uc801\uc778 CNN\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from six.moves import urllib\nopener = urllib.request.build_opener()\nopener.addheaders = [('User-agent', 'Mozilla/5.0')]\nurllib.request.install_opener(opener)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# \ud559\uc2b5\uc6a9 \ub370\uc774\ud130\uc14b\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize((0.1307,), (0.3081,))\n                   ])), batch_size=64, shuffle=True, num_workers=4)\n# \ud14c\uc2a4\ud2b8\uc6a9 \ub370\uc774\ud130\uc14b\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(root='.', train=False, transform=transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,))\n    ])), batch_size=64, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spatial Transformer Networks(STN) \uad6c\uc131\ud558\uae30\n--------------------------------------\n\nSTN\uc740 \ub2e4\uc74c\uc758 \uc138 \uac00\uc9c0 \uc8fc\uc694 \uad6c\uc131 \uc694\uc18c\ub85c \uc694\uc57d\ub429\ub2c8\ub2e4.\n\n-  \uc704\uce58 \uacb0\uc815 \ub124\ud2b8\uc6cc\ud06c(localization network)\ub294 \uacf5\uac04 \ubcc0\ud658 \ud30c\ub77c\ubbf8\ud130\ub97c \uc608\uce21(regress)\n   \ud558\ub294 \uc77c\ubc18\uc801\uc778 CNN \uc785\ub2c8\ub2e4. \uacf5\uac04 \ubcc0\ud658\uc740 \ub370\uc774\ud130 \uc14b\uc73c\ub85c\ubd80\ud130 \uba85\uc2dc\uc801\uc73c\ub85c \ud559\uc2b5\ub418\uc9c0 \uc54a\uace0,\n   \uc2e0\uacbd\ub9dd\uc774 \uc804\uccb4 \uc815\ud655\ub3c4\ub97c \ud5a5\uc0c1 \uc2dc\ud0a4\ub3c4\ub85d \uacf5\uac04 \ubcc0\ud658\uc744 \uc790\ub3d9\uc73c\ub85c \ud559\uc2b5\ud569\ub2c8\ub2e4.\n-  \uadf8\ub9ac\ub4dc \uc0dd\uc131\uae30(grid generator)\ub294 \ucd9c\ub825 \uc774\ubbf8\uc9c0\ub85c\ubd80\ud130 \uac01 \ud53d\uc140\uc5d0 \ub300\uc751\ud558\ub294 \uc785\ub825\n   \uc774\ubbf8\uc9c0 \ub0b4 \uc88c\ud45c \uadf8\ub9ac\ub4dc\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n-  \uc0d8\ud50c\ub7ec(sampler)\ub294 \uacf5\uac04 \ubcc0\ud658 \ud30c\ub77c\ubbf8\ud130\ub97c \uc785\ub825 \uc774\ubbf8\uc9c0\uc5d0 \uc801\uc6a9\ud569\ub2c8\ub2e4.\n\n.. figure:: /_static/img/stn/stn-arch.png\n\n.. Note::\n   affine_grid \ubc0f grid_sample \ubaa8\ub4c8\uc774 \ud3ec\ud568\ub41c \ucd5c\uc2e0 \ubc84\uc804\uc758 PyTorch\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n        # \uacf5\uac04 \ubcc0\ud658\uc744 \uc704\ud55c \uc704\uce58 \uacb0\uc815 \ub124\ud2b8\uc6cc\ud06c (localization-network)\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        # [3 * 2] \ud06c\uae30\uc758 \uc544\ud540(affine) \ud589\ub82c\uc5d0 \ub300\ud574 \uc608\uce21\n        self.fc_loc = nn.Sequential(\n            nn.Linear(10 * 3 * 3, 32),\n            nn.ReLU(True),\n            nn.Linear(32, 3 * 2)\n        )\n\n        # \ud56d\ub4f1 \ubcc0\ud658(identity transformation)\uc73c\ub85c \uac00\uc911\uce58/\ubc14\uc774\uc5b4\uc2a4 \ucd08\uae30\ud654\n        self.fc_loc[2].weight.data.zero_()\n        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n\n    # STN\uc758 forward \ud568\uc218\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 2, 3)\n\n        grid = F.affine_grid(theta, x.size())\n        x = F.grid_sample(x, grid)\n\n        return x\n\n    def forward(self, x):\n        # \uc785\ub825\uc744 \ubcc0\ud658\n        x = self.stn(x)\n\n        # \uc77c\ubc18\uc801\uc778 forward pass\ub97c \uc218\ud589\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\nmodel = Net().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \ud559\uc2b5\ud558\uae30\n------------------\n\n\uc774\uc81c SGD \uc54c\uace0\ub9ac\uc998\uc744 \uc774\uc6a9\ud574 \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ucf1c \ubd05\uc2dc\ub2e4. \uc55e\uc11c \uad6c\uc131\ud55c \uc2e0\uacbd\ub9dd\uc740\n\uac10\ub3c5 \ud559\uc2b5 \ubc29\uc2dd(supervised way)\uc73c\ub85c \ubd84\ub958 \ubb38\uc81c\ub97c \ud559\uc2b5\ud569\ub2c8\ub2e4. \ub610\ud55c \uc774 \ubaa8\ub378\uc740\nend-to-end \ubc29\uc2dd\uc73c\ub85c STN\uc744 \uc790\ub3d9\uc73c\ub85c \ud559\uc2b5\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 500 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n#\n# MNIST \ub370\uc774\ud130\uc14b\uc5d0\uc11c STN\uc758 \uc131\ub2a5\uc744 \uce21\uc815\ud558\uae30 \uc704\ud55c \uac04\ub2e8\ud55c \ud14c\uc2a4\ud2b8 \uc808\ucc28\n#\n\n\ndef test():\n    with torch.no_grad():\n        model.eval()\n        test_loss = 0\n        correct = 0\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n\n            # \ubc30\uce58 \uc190\uc2e4 \ud569\ud558\uae30\n            test_loss += F.nll_loss(output, target, size_average=False).item()\n            # \ub85c\uadf8-\ud655\ub960\uc758 \ucd5c\ub300\uac12\uc5d0 \ud574\ub2f9\ud558\ub294 \uc778\ub371\uc2a4 \uac00\uc838\uc624\uae30\n            pred = output.max(1, keepdim=True)[1]\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n        test_loss /= len(test_loader.dataset)\n        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n              .format(test_loss, correct, len(test_loader.dataset),\n                      100. * correct / len(test_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "STN \uacb0\uacfc \uc2dc\uac01\ud654\ud558\uae30\n---------------------------\n\n\uc774\uc81c \ud559\uc2b5\ud55c \ube44\uc8fc\uc5bc \uc5b4\ud150\uc158 \uba54\ucee4\ub2c8\uc998\uc758 \uacb0\uacfc\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ud559\uc2b5\ud558\ub294 \ub3d9\uc548 \ubcc0\ud658\ub41c \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\ud558\uae30 \uc704\ud574 \uc791\uc740 \ub3c4\uc6c0(helper) \ud568\uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def convert_image_np(inp):\n    \"\"\"Convert a Tensor to numpy image.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    return inp\n\n# \ud559\uc2b5 \ud6c4 \uacf5\uac04 \ubcc0\ud658 \uacc4\uce35\uc758 \ucd9c\ub825\uc744 \uc2dc\uac01\ud654\ud558\uace0, \uc785\ub825 \uc774\ubbf8\uc9c0 \ubc30\uce58 \ub370\uc774\ud130 \ubc0f\n# STN\uc744 \uc0ac\uc6a9\ud574 \ubcc0\ud658\ub41c \ubc30\uce58 \ub370\uc774\ud130\ub97c \uc2dc\uac01\ud654 \ud569\ub2c8\ub2e4.\n\n\ndef visualize_stn():\n    with torch.no_grad():\n        # \ud559\uc2b5 \ub370\uc774\ud130\uc758 \ubc30\uce58 \uac00\uc838\uc624\uae30\n        data = next(iter(test_loader))[0].to(device)\n\n        input_tensor = data.cpu()\n        transformed_input_tensor = model.stn(data).cpu()\n\n        in_grid = convert_image_np(\n            torchvision.utils.make_grid(input_tensor))\n\n        out_grid = convert_image_np(\n            torchvision.utils.make_grid(transformed_input_tensor))\n\n        # \uacb0\uacfc\ub97c \ub098\ub780\ud788 \ud45c\uc2dc\ud558\uae30\n        f, axarr = plt.subplots(1, 2)\n        axarr[0].imshow(in_grid)\n        axarr[0].set_title('Dataset Images')\n\n        axarr[1].imshow(out_grid)\n        axarr[1].set_title('Transformed Images')\n\nfor epoch in range(1, 20 + 1):\n    train(epoch)\n    test()\n\n# \uc77c\ubd80 \uc785\ub825 \ubc30\uce58 \ub370\uc774\ud130\uc5d0\uc11c STN \ubcc0\ud658 \uacb0\uacfc\ub97c \uc2dc\uac01\ud654\nvisualize_stn()\n\nplt.ioff()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}