{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n(\ubca0\ud0c0) LSTM \uae30\ubc18 \ub2e8\uc5b4 \ub2e8\uc704 \uc5b8\uc5b4 \ubaa8\ub378\uc758 \ub3d9\uc801 \uc591\uc790\ud654\n==================================================================\n\n**Author**: `James Reed <https://github.com/jamesr66a>`_\n\n**Edited by**: `Seth Weidman <https://github.com/SethHWeidman/>`_\n\n**\ubc88\uc5ed**: `\ubc15\uacbd\ub9bc <https://github.com/kypark7/>`_ `Myungha Kwon <https://github.com/kwonmha/>`_\n\n\uc2dc\uc791\ud558\uae30\n------------\n\n\uc591\uc790\ud654\ub294 \ubaa8\ub378\uc758 \ud06c\uae30\ub97c \uc904\uc774\uace0 \ucd94\ub860 \uc18d\ub3c4\ub97c \ub192\uc774\uba74\uc11c\ub3c4 \uc815\ud655\ub3c4\ub294 \ubcc4\ub85c \ub0ae\uc544\uc9c0\uc9c0 \uc54a\ub3c4\ub85d,\n\ubaa8\ub378\uc758 \uac00\uc911\uce58\uc640 \ud65c\uc131 \ud568\uc218\ub97c \uc2e4\uc218\ud615\uc5d0\uc11c \uc815\uc218\ud615\uc73c\ub85c \ubcc0\ud658\ud569\ub2c8\ub2e4.\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 PyTorch\uc758 `\ub2e8\uc5b4 \ub2e8\uc704 \uc5b8\uc5b4 \ubaa8\ub378 <https://github.com/pytorch/examples/tree/master/word_language_model>`_\n\uc608\uc81c\ub97c \ub530\ub77c\ud558\uba74\uc11c, LSTM \uae30\ubc18\uc758 \ub2e8\uc5b4 \uc608\uce21 \ubaa8\ub378\uc5d0 \uac00\uc7a5 \uac04\ub2e8\ud55c \uc591\uc790\ud654 \uae30\ubc95\uc778\n`\ub3d9\uc801 \uc591\uc790\ud654 <https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic>`_\n\ub97c \uc801\uc6a9\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \ubd88\ub7ec\uc624\uae30\nimport os\nfrom io import open\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. \ubaa8\ub378 \uc815\uc758\ud558\uae30\n-------------------\n\n\ub2e8\uc5b4 \ub2e8\uc704 \uc5b8\uc5b4 \ubaa8\ub378 \uc608\uc81c\uc5d0\uc11c \uc0ac\uc6a9\ub41c `\ubaa8\ub378 <https://github.com/pytorch/examples/blob/master/word_language_model/model.py>`_ \uc744\n\ub530\ub77c LSTM \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LSTMModel(nn.Module):\n    \"\"\"\uc778\ucf54\ub354, \ubc18\ubcf5 \ubaa8\ub4c8 \ubc0f \ub514\ucf54\ub354\uac00 \uc788\ub294 \ucee8\ud14c\uc774\ub108 \ubaa8\ub4c8.\"\"\"\n\n    def __init__(self, ntoken, ninp, nhid, nlayers, dropout=0.5):\n        super(LSTMModel, self).__init__()\n        self.drop = nn.Dropout(dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n        self.decoder = nn.Linear(nhid, ntoken)\n\n        self.init_weights()\n\n        self.nhid = nhid\n        self.nlayers = nlayers\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output)\n        return decoded, hidden\n\n    def init_hidden(self, bsz):\n        weight = next(self.parameters())\n        return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n                weight.new_zeros(self.nlayers, bsz, self.nhid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \ud14d\uc2a4\ud2b8 \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\n------------------------\n\n\ub2e4\uc74c\uc73c\ub85c, \ub2e8\uc5b4 \ub2e8\uc704 \uc5b8\uc5b4 \ubaa8\ub378 \uc608\uc81c\uc758 `\uc804\ucc98\ub9ac <https://github.com/pytorch/examples/blob/master/word_language_model/data.py>`_\n\uacfc\uc815\uc744 \ub530\ub77c `Wikitext-2 \ub370\uc774\ud130\uc14b <https://www.google.com/search?q=wikitext+2+data>`_ \uc744 `Corpus` \uc778\uc2a4\ud134\uc2a4\uc5d0 \ubd88\ub7ec\uc635\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Dictionary(object):\n    def __init__(self):\n        self.word2idx = {}\n        self.idx2word = []\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\nclass Corpus(object):\n    def __init__(self, path):\n        self.dictionary = Dictionary()\n        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n\n    def tokenize(self, path):\n        assert os.path.exists(path)\n        \"\"\"\ud14d\uc2a4\ud2b8 \ud30c\uc77c \ud1a0\ud070\ud654\"\"\"\n        assert os.path.exists(path)\n        # \uc0ac\uc804\uc5d0 \ub2e8\uc5b4 \ucd94\uac00\n        with open(path, 'r', encoding=\"utf8\") as f:\n            for line in f:\n                words = line.split() + ['<eos>']\n                for word in words:\n                    self.dictionary.add_word(word)\n\n        # \ud30c\uc77c \ub0b4\uc6a9 \ud1a0\ud070\ud654\n        with open(path, 'r', encoding=\"utf8\") as f:\n            idss = []\n            for line in f:\n                words = line.split() + ['<eos>']\n                ids = []\n                for word in words:\n                    ids.append(self.dictionary.word2idx[word])\n                idss.append(torch.tensor(ids).type(torch.int64))\n            ids = torch.cat(idss)\n\n        return ids\n\nmodel_data_filepath = 'data/'\n\ncorpus = Corpus(model_data_filepath + 'wikitext-2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378 \ubd88\ub7ec\uc624\uae30\n-----------------------------\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 \ubaa8\ub378\uc774 \ud559\uc2b5\ub41c \ud6c4 \uc801\uc6a9\ub418\ub294 \uc591\uc790\ud654 \uae30\uc220\uc778 \ub3d9\uc801 \uc591\uc790\ud654\uc5d0 \ub300\ud55c \ud29c\ud1a0\ub9ac\uc5bc\uc785\ub2c8\ub2e4.\n\ub530\ub77c\uc11c \uc6b0\ub9ac\ub294 \ubbf8\ub9ac \ud559\uc2b5\ub41c \uac00\uc911\uce58\ub97c \ubaa8\ub378 \uc544\ud0a4\ud14d\ucc98\uc5d0 \ub85c\ub4dc\ud560 \uac83 \uc785\ub2c8\ub2e4. \uc774 \uac00\uc911\uce58\ub294 word\nlanguage \ubaa8\ub378 \uc608\uc81c\uc758 \uae30\ubcf8 \uc124\uc815\uc744 \uc0ac\uc6a9\ud558\uc5ec 5\uac1c\uc758 epoch \ub3d9\uc548 \ud559\uc2b5\ud558\uc5ec \uc5bb\uc740 \uac83\uc785\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntokens = len(corpus.dictionary)\n\nmodel = LSTMModel(\n    ntoken = ntokens,\n    ninp = 512,\n    nhid = 256,\n    nlayers = 5,\n)\n\nmodel.load_state_dict(\n    torch.load(\n        model_data_filepath + 'word_language_model_quantize.pth',\n        map_location=torch.device('cpu')\n        )\n    )\n\nmodel.eval()\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \uc0ac\uc804 \ud559\uc2b5\ub41c \ubaa8\ub378\uc774 \uc798 \ub3d9\uc791\ud558\ub294\uc9c0 \ud655\uc778\ud574\ubcf4\uae30 \uc704\ud574 \ud14d\uc2a4\ud2b8\ub97c \uc0dd\uc131\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\uc9c0\uae08\uae4c\uc9c0 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc9c4\ud589\ud588\ub358 \ubc29\uc2dd\ucc98\ub7fc `\uc774 \uc608\uc81c <https://github.com/pytorch/examples/blob/master/word_language_model/generate.py>`_ \ub97c \ub530\ub77c \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_ = torch.randint(ntokens, (1, 1), dtype=torch.long)\nhidden = model.init_hidden(1)\ntemperature = 1.0\nnum_words = 1000\n\nwith open(model_data_filepath + 'out.txt', 'w') as outf:\n    with torch.no_grad():  # \uae30\ub85d\uc744 \ucd94\uc801\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n        for i in range(num_words):\n            output, hidden = model(input_, hidden)\n            word_weights = output.squeeze().div(temperature).exp().cpu()\n            word_idx = torch.multinomial(word_weights, 1)[0]\n            input_.fill_(word_idx)\n\n            word = corpus.dictionary.idx2word[word_idx]\n\n            outf.write(str(word.encode('utf-8')) + ('\\n' if i % 20 == 19 else ' '))\n\n            if i % 100 == 0:\n                print('| Generated {}/{} words'.format(i, 1000))\n\nwith open(model_data_filepath + 'out.txt', 'r') as outf:\n    all_output = outf.read()\n    print(all_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774 \ubaa8\ub378\uc774 GPT-2\ub294 \uc544\ub2c8\uc9c0\ub9cc, \uc5b8\uc5b4\uc758 \uad6c\uc870\ub97c \ubc30\uc6b0\uae30 \uc2dc\uc791\ud55c \uac83\ucc98\ub7fc \ubcf4\uc785\ub2c8\ub2e4!\n\n\n\ub3d9\uc801 \uc591\uc790\ud654\ub97c \uc2dc\uc5f0\ud560 \uc900\ube44\uac00 \uac70\uc758 \ub05d\ub0ac\uc2b5\ub2c8\ub2e4. \uba87 \uac00\uc9c0 helper \ud568\uc218\ub97c \uc815\uc758\ud558\uae30\ub9cc \ud558\uba74 \ub429\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bptt = 25\ncriterion = nn.CrossEntropyLoss()\neval_batch_size = 1\n\n# \ud14c\uc2a4\ud2b8 \ub370\uc774\ud130\uc14b \ub9cc\ub4e4\uae30\ndef batchify(data, bsz):\n    # \ub370\uc774\ud130\uc14b\uc744 bsz \ubd80\ubd84\uc73c\ub85c \uc5bc\ub9c8\ub098 \uae54\ub054\ud558\uac8c \ub098\ub20c \uc218 \uc788\ub294\uc9c0 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n    nbatch = data.size(0) // bsz\n    # \uae54\ub054\ud558\uac8c \ub9de\uc9c0 \uc54a\ub294 \ucd94\uac00\uc801\uc778 \ubd80\ubd84(\ub098\uba38\uc9c0\ub4e4)\uc744 \uc798\ub77c\ub0c5\ub2c8\ub2e4.\n    data = data.narrow(0, 0, nbatch * bsz)\n    # \ub370\uc774\ud130\uc5d0 \ub300\ud558\uc5ec bsz \ubc30\uce58\ub4e4\ub85c \ub3d9\ub4f1\ud558\uac8c \ub098\ub215\ub2c8\ub2e4.\n    return data.view(bsz, -1).t().contiguous()\n\ntest_data = batchify(corpus.test, eval_batch_size)\n\n# \ud3c9\uac00 \ud568\uc218\ub4e4\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].reshape(-1)\n    return data, target\n\ndef repackage_hidden(h):\n  \"\"\"\uc740\ub2c9 \uc0c1\ud0dc\ub97c \ubcc0\ud654\ub3c4 \uae30\ub85d\uc5d0\uc11c \uc81c\uac70\ub41c \uc0c8\ub85c\uc6b4 tensor\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.\"\"\"\n\n  if isinstance(h, torch.Tensor):\n      return h.detach()\n  else:\n      return tuple(repackage_hidden(v) for v in h)\n\ndef evaluate(model_, data_source):\n    # Dropout\uc744 \uc911\uc9c0\uc2dc\ud0a4\ub294 \ud3c9\uac00 \ubaa8\ub4dc\ub85c \uc2e4\ud589\ud569\ub2c8\ub2e4.\n    model_.eval()\n    total_loss = 0.\n    hidden = model_.init_hidden(eval_batch_size)\n    with torch.no_grad():\n        for i in range(0, data_source.size(0) - 1, bptt):\n            data, targets = get_batch(data_source, i)\n            output, hidden = model_(data, hidden)\n            hidden = repackage_hidden(hidden)\n            output_flat = output.view(-1, ntokens)\n            total_loss += len(data) * criterion(output_flat, targets).item()\n    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \ub3d9\uc801 \uc591\uc790\ud654 \ud14c\uc2a4\ud2b8\ud558\uae30\n----------------------------\n\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c \ubaa8\ub378\uc5d0\uc11c ``torch.quantization.quantize_dynamic`` \uc744 \ud638\ucd9c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4!\n\uad6c\uccb4\uc801\uc73c\ub85c,\n\n- \ubaa8\ub378\uc758 ``nn.LSTM`` \uacfc ``nn.Linear`` \ubaa8\ub4c8\uc744 \uc591\uc790\ud654 \ud558\ub3c4\ub85d \uba85\uc2dc\ud569\ub2c8\ub2e4.\n- \uac00\uc911\uce58\ub4e4\uc774 ``int8`` \uac12\uc73c\ub85c \ubcc0\ud658\ub418\ub3c4\ub85d \uba85\uc2dc\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n)\nprint(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uc740 \ub3d9\uc77c\ud558\uac8c \ubcf4\uc785\ub2c8\ub2e4. \uc774\uac83\uc774 \uc5b4\ub5bb\uac8c \uc774\ub4dd\uc744 \uc8fc\ub294 \uac83\uc77c\uae4c\uc694? \uccab\uc9f8, \ubaa8\ub378 \ud06c\uae30\uac00\n\uc0c1\ub2f9\ud788 \uc904\uc5b4 \ub4ed\ub2c8\ub2e4:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_size_of_model(model):\n    torch.save(model.state_dict(), \"temp.p\")\n    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n    os.remove('temp.p')\n\nprint_size_of_model(model)\nprint_size_of_model(quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub450 \ubc88\uc9f8\ub85c, \ud3c9\uac00 \uc190\uc2e4\uac12\uc740 \uac19\uc73c\ub098 \ucd94\ub860(inference) \uc18d\ub3c4\uac00 \ube68\ub77c\uc84c\uc2b5\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \uba54\ubaa8: \uc591\uc790\ud654 \ub41c \ubaa8\ub378\uc740 \ub2e8\uc77c \uc2a4\ub808\ub4dc\ub85c \uc2e4\ud589\ub418\uae30 \ub54c\ubb38\uc5d0 \ub2e8\uc77c \uc2a4\ub808\ub4dc \ube44\uad50\ub97c \uc704\ud574\n# \uc2a4\ub808\ub4dc \uc218\ub97c 1\ub85c \uc124\uc815\ud588\uc2b5\ub2c8\ub2e4.\n\ntorch.set_num_threads(1)\n\ndef time_model_evaluation(model, test_data):\n    s = time.time()\n    loss = evaluate(model, test_data)\n    elapsed = time.time() - s\n    print('''loss: {0:.3f}\\nelapsed time (seconds): {1:.1f}'''.format(loss, elapsed))\n\ntime_model_evaluation(model, test_data)\ntime_model_evaluation(quantized_model, test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MacBook Pro\uc5d0\uc11c \ub85c\uceec\ub85c \uc2e4\ud589\ud558\ub294 \uacbd\uc6b0, \uc591\uc790\ud654 \uc5c6\uc774\ub294 \ucd94\ub860(inference)\uc5d0 \uc57d 200\ucd08\uac00 \uac78\ub9ac\uace0\n\uc591\uc790\ud654\ub97c \uc0ac\uc6a9\ud558\uba74 \uc57d 100\ucd08\uac00 \uac78\ub9bd\ub2c8\ub2e4.\n\n\ub9c8\uce58\uba70\n----------\n\n\ub3d9\uc801 \uc591\uc790\ud654\ub294 \uc815\ud655\ub3c4\uc5d0 \uc81c\ud55c\uc801\uc778 \uc601\ud5a5\uc744 \ubbf8\uce58\uba74\uc11c \ubaa8\ub378 \ud06c\uae30\ub97c \uc904\uc774\ub294\n\uc26c\uc6b4 \ubc29\ubc95\uc774 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc77d\uc5b4\uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4. \uc5b8\uc81c\ub098\ucc98\ub7fc \uc5b4\ub5a0\ud55c \ud53c\ub4dc\ubc31\ub3c4 \ud658\uc601\uc774\ub2c8, \uc758\uacac\uc774 \uc788\ub2e4\uba74\n`\uc5ec\uae30 <https://github.com/pytorch/pytorch/issues>`_ \uc5d0 \uc774\uc288\ub97c \ub0a8\uaca8 \uc8fc\uc138\uc694.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}