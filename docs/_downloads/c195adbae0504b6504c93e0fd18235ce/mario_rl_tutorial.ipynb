{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\ub9c8\ub9ac\uc624 \uac8c\uc784 RL \uc5d0\uc774\uc804\ud2b8\ub85c \ud559\uc2b5\ud558\uae30\n===============================\n\n\uc800\uc790: `Yuansong Feng <https://github.com/YuansongFeng>`__, `Suraj\nSubramanian <https://github.com/suraj813>`__, `Howard\nWang <https://github.com/hw26>`__, `Steven\nGuo <https://github.com/GuoYuzhang>`__.\n\n\ubc88\uc5ed: `\uae40\ud0dc\uc601 <https://github.com/Taeyoung96>`__.  \n\n\uc774\ubc88 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 \uc2ec\uce35 \uac15\ud654 \ud559\uc2b5\uc758 \uae30\ubcf8 \uc0ac\ud56d\ub4e4\uc5d0 \ub300\ud574 \uc774\uc57c\uae30\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\ub9c8\uc9c0\ub9c9\uc5d0\ub294, \uc2a4\uc2a4\ub85c \uac8c\uc784\uc744 \ud560 \uc218 \uc788\ub294 AI \uae30\ubc18 \ub9c8\ub9ac\uc624\ub97c \n(`Double Deep Q-Networks <https://arxiv.org/pdf/1509.06461.pdf>`__ \uc0ac\uc6a9) \n\uad6c\ud604\ud558\uac8c \ub429\ub2c8\ub2e4.\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 RL\uc5d0 \ub300\ud55c \uc0ac\uc804 \uc9c0\uc2dd\uc774 \ud544\uc694\ud558\uc9c0 \uc54a\uc9c0\ub9cc, \n\uc774\ub7ec\ud55c `\ub9c1\ud06c <https://spinningup.openai.com/en/latest/spinningup/rl_intro.html>`__\n\ub97c \ud1b5\ud574 RL \uac1c\ub150\uc5d0 \uce5c\uc219\ud574 \uc9c8 \uc218 \uc788\uc73c\uba70,\n\uc5ec\uae30 \uc788\ub294\n`\uce58\ud2b8\uc2dc\ud2b8 <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N>`__\n\ub97c \ud65c\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c \uc0ac\uc6a9\ud558\ub294 \uc804\uccb4 \ucf54\ub4dc\ub294\n`\uc5ec\uae30 <https://github.com/yuansongFeng/MadMario/>`__\n\uc5d0\uc11c \ud655\uc778 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n.. figure:: /_static/img/mario.gif\n   :alt: mario\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# !pip install gym-super-mario-bros==7.3.0\n\nimport torch\nfrom torch import nn\nfrom torchvision import transforms as T\nfrom PIL import Image\nimport numpy as np\nfrom pathlib import Path\nfrom collections import deque\nimport random, datetime, os, copy\n\n# Gym\uc740 \uac15\ud654\ud559\uc2b5\uc744 \uc704\ud55c OpenAI \ud234\ud0b7\uc785\ub2c8\ub2e4.\nimport gym\nfrom gym.spaces import Box\nfrom gym.wrappers import FrameStack\n\n# OpenAI Gym\uc744 \uc704\ud55c NES \uc5d0\ubbac\ub808\uc774\ud130\nfrom nes_py.wrappers import JoypadSpace\n\n# OpenAI Gym\uc5d0\uc11c\uc758 \uc288\ud37c \ub9c8\ub9ac\uc624 \ud658\uacbd \uc138\ud305\nimport gym_super_mario_bros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac15\ud654\ud559\uc2b5 \uac1c\ub150\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n**\ud658\uacbd(Environment)** : \uc5d0\uc774\uc804\ud2b8\uac00 \uc0c1\ud638\uc791\uc6a9\ud558\uba70 \uc2a4\uc2a4\ub85c \ubc30\uc6b0\ub294 \uc138\uacc4\uc785\ub2c8\ub2e4.\n\n**\ud589\ub3d9(Action)** $a$ : \uc5d0\uc774\uc804\ud2b8\uac00 \ud658\uacbd\uc5d0 \uc5b4\ub5bb\uac8c \uc751\ub2f5\ud558\ub294\uc9c0 \ud589\ub3d9\uc744 \ud1b5\ud574 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \n\uac00\ub2a5\ud55c \ubaa8\ub4e0 \ud589\ub3d9\uc758 \uc9d1\ud569\uc744 *\ud589\ub3d9 \uacf5\uac04* \uc774\ub77c\uace0 \ud569\ub2c8\ub2e4.\n\n**\uc0c1\ud0dc(State)** $s$ : \ud658\uacbd\uc758 \ud604\uc7ac \ud2b9\uc131\uc744 \uc0c1\ud0dc\ub97c \ud1b5\ud574 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\ud658\uacbd\uc774 \uc788\uc744 \uc218 \uc788\ub294 \ubaa8\ub4e0 \uac00\ub2a5\ud55c \uc0c1\ud0dc \uc9d1\ud569\uc744 *\uc0c1\ud0dc \uacf5\uac04* \uc774\ub77c\uace0 \ud569\ub2c8\ub2e4.\n\n**\ud3ec\uc0c1(Reward)** $r$ : \ud3ec\uc0c1\uc740 \ud658\uacbd\uc5d0\uc11c \uc5d0\uc774\uc804\ud2b8\ub85c \uc804\ub2ec\ub418\ub294 \ud575\uc2ec \ud53c\ub4dc\ubc31\uc785\ub2c8\ub2e4.\n\uc5d0\uc774\uc804\ud2b8\uac00 \ud559\uc2b5\ud558\uace0 \ud5a5\ud6c4 \ud589\ub3d9\uc744 \ubcc0\uacbd\ud558\ub3c4\ub85d \uc720\ub3c4\ud558\ub294 \uac83\uc785\ub2c8\ub2e4.\n\uc5ec\ub7ec \uc2dc\uac04 \ub2e8\uacc4\uc5d0 \uac78\uce5c \ud3ec\uc0c1\uc758 \ud569\uc744 **\ub9ac\ud134(Return)** \uc774\ub77c\uace0 \ud569\ub2c8\ub2e4.\n\n**\ucd5c\uc801\uc758 \ud589\ub3d9-\uac00\uce58 \ud568\uc218(Action-Value function)** $Q^*(s,a)$ : \uc0c1\ud0dc $s$\n\uc5d0\uc11c \uc2dc\uc791\ud558\uba74 \uc608\uc0c1\ub418\ub294 \ub9ac\ud134\uc744 \ubc18\ud658\ud558\uace0, \uc784\uc758\uc758 \ud589\ub3d9 $a$\n\ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uac01\uac01\uc758 \ubbf8\ub798\uc758 \ub2e8\uacc4\uc5d0\uc11c \ud3ec\uc0c1\uc758 \ud569\uc744 \uadf9\ub300\ud654\ud558\ub294 \ud589\ub3d9\uc744 \uc120\ud0dd\ud558\ub3c4\ub85d \ud569\ub2c8\ub2e4.\n$Q$ \ub294 \uc0c1\ud0dc\uc5d0\uc11c \ud589\ub3d9\uc758 \u201c\ud488\uc9c8\u201d \n\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \uc774 \ud568\uc218\ub97c \uadfc\uc0ac \uc2dc\ud0a4\ub824\uace0 \ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud658\uacbd(Environment)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n\ud658\uacbd \ucd08\uae30\ud654\ud558\uae30\n------------------------\n\n\ub9c8\ub9ac\uc624 \uac8c\uc784\uc5d0\uc11c \ud658\uacbd\uc740 \ud29c\ube0c, \ubc84\uc12f, \uadf8 \uc774\uc678 \ub2e4\ub978 \uc5ec\ub7ec \uc694\uc18c\ub4e4\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ub9c8\ub9ac\uc624\uac00 \ud589\ub3d9\uc744 \ucde8\ud558\uba74, \ud658\uacbd\uc740 \ubcc0\uacbd\ub41c (\ub2e4\uc74c)\uc0c1\ud0dc, \ud3ec\uc0c1 \uadf8\ub9ac\uace0\n\ub2e4\ub978 \uc815\ubcf4\ub4e4\ub85c \uc751\ub2f5\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \uc288\ud37c \ub9c8\ub9ac\uc624 \ud658\uacbd \ucd08\uae30\ud654\ud558\uae30\nenv = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n\n# \uc0c1\ud0dc \uacf5\uac04\uc744 2\uac00\uc9c0\ub85c \uc81c\ud55c\ud558\uae30\n#   0. \uc624\ub978\ucabd\uc73c\ub85c \uac77\uae30\n#   1. \uc624\ub978\ucabd\uc73c\ub85c \uc810\ud504\ud558\uae30\nenv = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n\nenv.reset()\nnext_state, reward, done, info = env.step(action=0)\nprint(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud658\uacbd \uc804\ucc98\ub9ac \uacfc\uc815 \uac70\uce58\uae30\n------------------------\n\n``\ub2e4\uc74c \uc0c1\ud0dc(next_state)`` \uc5d0\uc11c \ud658\uacbd \ub370\uc774\ud130\uac00 \uc5d0\uc774\uc804\ud2b8\ub85c \ubc18\ud658\ub429\ub2c8\ub2e4.\n\uc55e\uc11c \uc0b4\ud3b4\ubcf4\uc558\ub4ef\uc774, \uac01\uac01\uc758 \uc0c1\ud0dc\ub294 ``[3, 240, 256]`` \uc758 \ubc30\uc5f4\ub85c \ub098\ud0c0\ub0b4\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\uc885\uc885 \uc0c1\ud0dc\uac00 \uc81c\uacf5\ud558\ub294 \uac83\uc740 \uc5d0\uc774\uc804\ud2b8\uac00 \ud544\uc694\ub85c \ud558\ub294 \uac83\ubcf4\ub2e4 \ub354 \ub9ce\uc740 \uc815\ubcf4\uc785\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uc5b4, \ub9c8\ub9ac\uc624\uc758 \ud589\ub3d9\uc740 \ud30c\uc774\ud504\uc758 \uc0c9\uae54\uc774\ub098 \ud558\ub298\uc758 \uc0c9\uae54\uc5d0 \uc88c\uc6b0\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4!\n\n\uc544\ub798\uc5d0 \uc124\uba85\ud560 \ud074\ub798\uc2a4\ub4e4\uc740 \ud658\uacbd \ub370\uc774\ud130\ub97c \uc5d0\uc774\uc804\ud2b8\uc5d0 \ubcf4\ub0b4\uae30 \uc804 \ub2e8\uacc4\uc5d0\uc11c \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0 \uc0ac\uc6a9\ud560\n**\ub798\ud37c(Wrappers)** \uc785\ub2c8\ub2e4.\n\n``GrayScaleObservation`` \uc740 RGB \uc774\ubbf8\uc9c0\ub97c \ud751\ubc31 \uc774\ubbf8\uc9c0\ub85c \ubc14\uafb8\ub294 \uc77c\ubc18\uc801\uc778 \ub798\ud37c\uc785\ub2c8\ub2e4.\n``GrayScaleObservation`` \ud074\ub798\uc2a4\ub97c \uc0ac\uc6a9\ud558\uba74 \uc720\uc6a9\ud55c \uc815\ubcf4\ub97c \uc783\uc9c0 \uc54a\uace0 \uc0c1\ud0dc\uc758 \ud06c\uae30\ub97c \uc904\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n``GrayScaleObservation`` \ub97c \uc801\uc6a9\ud558\uba74 \uac01\uac01 \uc0c1\ud0dc\uc758 \ud06c\uae30\ub294\n``[1, 240, 256]`` \uc774 \ub429\ub2c8\ub2e4.\n\n``ResizeObservation`` \uc740 \uac01\uac01\uc758 \uc0c1\ud0dc(Observation)\ub97c \uc815\uc0ac\uac01\ud615 \uc774\ubbf8\uc9c0\ub85c \ub2e4\uc6b4 \uc0d8\ud50c\ub9c1\ud569\ub2c8\ub2e4.\n\uc774 \ub798\ud37c\ub97c \uc801\uc6a9\ud558\uba74 \uac01\uac01 \uc0c1\ud0dc\uc758 \ud06c\uae30\ub294 ``[1, 84, 84]`` \uc774 \ub429\ub2c8\ub2e4.\n\n``SkipFrame`` \uc740 ``gym.Wrapper`` \uc73c\ub85c\ubd80\ud130 \uc0c1\uc18d\uc744 \ubc1b\uc740 \uc0ac\uc6a9\uc790 \uc9c0\uc815 \ud074\ub798\uc2a4\uc774\uace0,\n``step()`` \ud568\uc218\ub97c \uad6c\ud604\ud569\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 \uc5f0\uc18d\ub418\ub294 \ud504\ub808\uc784\uc740 \ud070 \ucc28\uc774\uac00 \uc5c6\uae30 \ub54c\ubb38\uc5d0\nn\uac1c\uc758 \uc911\uac04 \ud504\ub808\uc784\uc744 \ud070 \uc815\ubcf4\uc758 \uc190\uc2e4 \uc5c6\uc774 \uac74\ub108\ub6f8 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\nn\ubc88\uc9f8 \ud504\ub808\uc784\uc740 \uac74\ub108\ub6f4 \uac01 \ud504\ub808\uc784\uc5d0 \uac78\uccd0 \ub204\uc801\ub41c \ud3ec\uc0c1\uc744\n\uc9d1\uacc4\ud569\ub2c8\ub2e4.\n\n``FrameStack`` \uc740 \ud658\uacbd\uc758 \uc5f0\uc18d \ud504\ub808\uc784\uc744\n\ub2e8\uc77c \uad00\ucc30 \uc9c0\uc810\uc73c\ub85c \ubc14\uafb8\uc5b4 \ud559\uc2b5 \ubaa8\ub378\uc5d0 \uc81c\uacf5\ud560 \uc218 \uc788\ub294 \ub798\ud37c\uc785\ub2c8\ub2e4.\n\uc774\ub807\uac8c \ud558\uba74 \ub9c8\ub9ac\uc624\uac00 \ucc29\uc9c0 \uc911\uc774\uc600\ub294\uc9c0 \ub610\ub294 \uc810\ud504 \uc911\uc774\uc5c8\ub294\uc9c0\n\uc774\uc804 \uba87 \ud504\ub808\uc784\uc758 \uc6c0\uc9c1\uc784 \ubc29\ud5a5\uc5d0 \ub530\ub77c \ud655\uc778\ud560 \uc218\n\uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n    def __init__(self, env, skip):\n        \"\"\"\ubaa8\ub4e0 `skip` \ud504\ub808\uc784\ub9cc \ubc18\ud658\ud569\ub2c8\ub2e4.\"\"\"\n        super().__init__(env)\n        self._skip = skip\n\n    def step(self, action):\n        \"\"\"\ud589\ub3d9\uc744 \ubc18\ubcf5\ud558\uace0 \ud3ec\uc0c1\uc744 \ub354\ud569\ub2c8\ub2e4.\"\"\"\n        total_reward = 0.0\n        done = False\n        for i in range(self._skip):\n            # \ud3ec\uc0c1\uc744 \ub204\uc801\ud558\uace0 \ub3d9\uc77c\ud55c \uc791\uc5c5\uc744 \ubc18\ubcf5\ud569\ub2c8\ub2e4.\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n            if done:\n                break\n        return obs, total_reward, done, info\n\n\nclass GrayScaleObservation(gym.ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n        obs_shape = self.observation_space.shape[:2]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def permute_orientation(self, observation):\n        # [H, W, C] \ubc30\uc5f4\uc744 [C, H, W] \ud150\uc11c\ub85c \ubc14\uafc9\ub2c8\ub2e4.\n        observation = np.transpose(observation, (2, 0, 1))\n        observation = torch.tensor(observation.copy(), dtype=torch.float)\n        return observation\n\n    def observation(self, observation):\n        observation = self.permute_orientation(observation)\n        transform = T.Grayscale()\n        observation = transform(observation)\n        return observation\n\n\nclass ResizeObservation(gym.ObservationWrapper):\n    def __init__(self, env, shape):\n        super().__init__(env)\n        if isinstance(shape, int):\n            self.shape = (shape, shape)\n        else:\n            self.shape = tuple(shape)\n\n        obs_shape = self.shape + self.observation_space.shape[2:]\n        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n\n    def observation(self, observation):\n        transforms = T.Compose(\n            [T.Resize(self.shape), T.Normalize(0, 255)]\n        )\n        observation = transforms(observation).squeeze(0)\n        return observation\n\n\n# \ub798\ud37c\ub97c \ud658\uacbd\uc5d0 \uc801\uc6a9\ud569\ub2c8\ub2e4.\nenv = SkipFrame(env, skip=4)\nenv = GrayScaleObservation(env)\nenv = ResizeObservation(env, shape=84)\nenv = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc55e\uc11c \uc18c\uac1c\ud55c \ub798\ud37c\ub97c \ud658\uacbd\uc5d0 \uc801\uc6a9\ud55c \ud6c4,\n\ucd5c\uc885 \ub798\ud551 \uc0c1\ud0dc\ub294 \uc67c\ucabd \uc544\ub798 \uc774\ubbf8\uc9c0\uc5d0 \ud45c\uc2dc\ub41c \uac83\ucc98\ub7fc 4\uac1c\uc758 \uc5f0\uc18d\ub41c \ud751\ubc31 \ud504\ub808\uc784\uc73c\ub85c \n\uad6c\uc131\ub429\ub2c8\ub2e4. \ub9c8\ub9ac\uc624\uac00 \ud589\ub3d9\uc744 \ud560 \ub54c\ub9c8\ub2e4,\n\ud658\uacbd\uc740 \uc774 \uad6c\uc870\uc758 \uc0c1\ud0dc\ub85c \uc751\ub2f5\ud569\ub2c8\ub2e4.\n\uad6c\uc870\ub294 ``[4, 84, 84]`` \ud06c\uae30\uc758 3\ucc28\uc6d0 \ubc30\uc5f4\ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\n.. figure:: /_static/img/mario_env.png\n   :alt: picture\n\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc5d0\uc774\uc804\ud2b8(Agent)\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n``Mario`` \ub77c\ub294 \ud074\ub798\uc2a4\ub97c \uc774 \uac8c\uc784\uc758 \uc5d0\uc774\uc804\ud2b8\ub85c \uc0dd\uc131\ud569\ub2c8\ub2e4.\n\ub9c8\ub9ac\uc624\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \uae30\ub2a5\uc744 \ud560 \uc218 \uc788\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n\n-  **\ud589\ub3d9(Act)** \uc740 (\ud658\uacbd\uc758) \ud604\uc7ac \uc0c1\ud0dc\ub97c \uae30\ubc18\uc73c\ub85c \n   \ucd5c\uc801\uc758 \ud589\ub3d9 \uc815\ucc45\uc5d0 \ub530\ub77c \uc815\ud574\uc9d1\ub2c8\ub2e4.\n\n-  \uacbd\ud5d8\uc744 **\uae30\uc5b5(Remember)** \ud558\ub294 \uac83. \n   \uacbd\ud5d8\uc740 (\ud604\uc7ac \uc0c1\ud0dc, \ud604\uc7ac \ud589\ub3d9, \ud3ec\uc0c1, \ub2e4\uc74c \uc0c1\ud0dc) \ub85c \uc774\ub8e8\uc5b4\uc838 \uc788\uc2b5\ub2c8\ub2e4. \n   \ub9c8\ub9ac\uc624\ub294 \uadf8\uc758 \ud589\ub3d9 \uc815\ucc45\uc744 \uc5c5\ub370\uc774\ud2b8 \ud558\uae30 \uc704\ud574  *\uce90\uc2dc(caches)* \ub97c \ud55c \ub2e4\uc74c, \uadf8\uc758 \uacbd\ud5d8\uc744 *\ub9ac\ucf5c(recalls)* \ud569\ub2c8\ub2e4.\n\n-  **\ud559\uc2b5(Learn)** \uc744 \ud1b5\ud574 \uc2dc\uac04\uc774 \uc9c0\ub0a8\uc5d0 \ub530\ub77c \ub354 \ub098\uc740 \ud589\ub3d9 \uc815\ucc45\uc744 \ud0dd\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n    def __init__():\n        pass\n\n    def act(self, state):\n        \"\"\"\uc0c1\ud0dc\uac00 \uc8fc\uc5b4\uc9c0\uba74, \uc785\uc2e4\ub860-\uadf8\ub9ac\ub514 \ud589\ub3d9(epsilon-greedy action)\uc744 \uc120\ud0dd\ud574\uc57c \ud569\ub2c8\ub2e4.\"\"\"\n        pass\n\n    def cache(self, experience):\n        \"\"\"\uba54\ubaa8\ub9ac\uc5d0 \uacbd\ud5d8\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4.\"\"\"\n        pass\n\n    def recall(self):\n        \"\"\"\uba54\ubaa8\ub9ac\ub85c\ubd80\ud130 \uacbd\ud5d8\uc744 \uc0d8\ud50c\ub9c1\ud569\ub2c8\ub2e4.\"\"\"\n        pass\n\n    def learn(self):\n        \"\"\"\uc77c\ub828\uc758 \uacbd\ud5d8\ub4e4\ub85c \uc2e4\uc2dc\uac04 \ud589\ub3d9 \uac00\uce58(online action value) (Q) \ud568\uc218\ub97c \uc5c5\ub370\uc774\ud2b8 \ud569\ub2c8\ub2e4.\"\"\"\n        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\ubc88 \uc139\uc158\uc5d0\uc11c\ub294 \ub9c8\ub9ac\uc624 \ud074\ub798\uc2a4\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \ucc44\uc6b0\uace0, \n\ub9c8\ub9ac\uc624 \ud074\ub798\uc2a4\uc758 \ud568\uc218\ub4e4\uc744 \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud589\ub3d9\ud558\uae30(Act)\n--------------\n\n\uc8fc\uc5b4\uc9c4 \uc0c1\ud0dc\uc5d0 \ub300\ud574, \uc5d0\uc774\uc804\ud2b8\ub294 \ucd5c\uc801\uc758 \ud589\ub3d9\uc744 \uc774\uc6a9\ud560 \uac83\uc778\uc9c0\n\uc784\uc758\uc758 \ud589\ub3d9\uc744 \uc120\ud0dd\ud558\uc5ec \ubd84\uc11d\ud560 \uac83\uc778\uc9c0 \uc120\ud0dd\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ub9c8\ub9ac\uc624\ub294 \uc784\uc758\uc758 \ud589\ub3d9\uc744 \uc120\ud0dd\ud588\uc744 \ub54c ``self.exploration_rate`` \ub97c \ud65c\uc6a9\ud569\ub2c8\ub2e4.\n\ucd5c\uc801\uc758 \ud589\ub3d9\uc744 \uc774\uc6a9\ud55c\ub2e4\uace0 \ud588\uc744 \ub54c, \uadf8\ub294 \ucd5c\uc801\uc758 \ud589\ub3d9\uc744 \uc218\ud589\ud558\uae30 \uc704\ud574   \n(``\ud559\uc2b5\ud558\uae30(Learn)`` \uc139\uc158\uc5d0\uc11c \uad6c\ud604\ub41c) ``MarioNet`` \uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n    def __init__(self, state_dim, action_dim, save_dir):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.save_dir = save_dir\n\n        self.use_cuda = torch.cuda.is_available()\n\n        # \ub9c8\ub9ac\uc624\uc758 DNN\uc740 \ucd5c\uc801\uc758 \ud589\ub3d9\uc744 \uc608\uce21\ud569\ub2c8\ub2e4 - \uc774\ub294 \ud559\uc2b5\ud558\uae30 \uc139\uc158\uc5d0\uc11c \uad6c\ud604\ud569\ub2c8\ub2e4.\n        self.net = MarioNet(self.state_dim, self.action_dim).float()\n        if self.use_cuda:\n            self.net = self.net.to(device=\"cuda\")\n\n        self.exploration_rate = 1\n        self.exploration_rate_decay = 0.99999975\n        self.exploration_rate_min = 0.1\n        self.curr_step = 0\n\n        self.save_every = 5e5  # Mario Net \uc800\uc7a5 \uc0ac\uc774\uc758 \uacbd\ud5d8 \ud69f\uc218\n\n    def act(self, state):\n        \"\"\"\n    \uc8fc\uc5b4\uc9c4 \uc0c1\ud0dc\uc5d0\uc11c, \uc785\uc2e4\ub860-\uadf8\ub9ac\ub514 \ud589\ub3d9(epsilon-greedy action)\uc744 \uc120\ud0dd\ud558\uace0, \uc2a4\ud15d\uc758 \uac12\uc744 \uc5c5\ub370\uc774\ud2b8 \ud569\ub2c8\ub2e4.\n\n    \uc785\ub825\uac12:\n    state(LazyFrame): \ud604\uc7ac \uc0c1\ud0dc\uc5d0\uc11c\uc758 \ub2e8\uc77c \uc0c1\ud0dc(observation)\uac12\uc744 \ub9d0\ud569\ub2c8\ub2e4. \ucc28\uc6d0\uc740 (state_dim)\uc785\ub2c8\ub2e4.\n    \ucd9c\ub825\uac12:\n    action_idx (int): Mario\uac00 \uc218\ud589\ud560 \ud589\ub3d9\uc744 \ub098\ud0c0\ub0b4\ub294 \uc815\uc218 \uac12\uc785\ub2c8\ub2e4.\n    \"\"\"\n        # \uc784\uc758\uc758 \ud589\ub3d9\uc744 \uc120\ud0dd\ud558\uae30\n        if np.random.rand() < self.exploration_rate:\n            action_idx = np.random.randint(self.action_dim)\n\n        # \ucd5c\uc801\uc758 \ud589\ub3d9\uc744 \uc774\uc6a9\ud558\uae30\n        else:\n            state = state.__array__()\n            if self.use_cuda:\n                state = torch.tensor(state).cuda()\n            else:\n                state = torch.tensor(state)\n            state = state.unsqueeze(0)\n            action_values = self.net(state, model=\"online\")\n            action_idx = torch.argmax(action_values, axis=1).item()\n\n        # exploration_rate \uac10\uc18c\ud558\uae30\n        self.exploration_rate *= self.exploration_rate_decay\n        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n\n        # \uc2a4\ud15d \uc218 \uc99d\uac00\ud558\uae30\n        self.curr_step += 1\n        return action_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uce90\uc2dc(Cache)\uc640 \ub9ac\ucf5c(Recall)\ud558\uae30\n------------------------------\n\n\uc774 \ub450\uac00\uc9c0 \ud568\uc218\ub294 \ub9c8\ub9ac\uc624\uc758 \u201c\uba54\ubaa8\ub9ac\u201d \ud504\ub85c\uc138\uc2a4 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.\n\n``cache()``: \ub9c8\ub9ac\uc624\uac00 \ud589\ub3d9\uc744 \ud560 \ub54c\ub9c8\ub2e4, \uadf8\ub294\n``\uacbd\ud5d8`` \uc744 \uadf8\uc758 \uba54\ubaa8\ub9ac\uc5d0 \uc800\uc7a5\ud569\ub2c8\ub2e4. \uadf8\uc758 \uacbd\ud5d8\uc5d0\ub294 \ud604\uc7ac *\uc0c1\ud0dc* \uc5d0 \ub530\ub978 \uc218\ud589\ub41c\n*\ud589\ub3d9* , \ud589\ub3d9\uc73c\ub85c\ubd80\ud130 \uc5bb\uc740 *\ud3ec\uc0c1* , *\ub2e4\uc74c \uc0c1\ud0dc*,\n\uadf8\ub9ac\uace0 \uac8c\uc784 *\uc644\ub8cc* \uc5ec\ubd80\uac00 \ud3ec\ud568\ub429\ub2c8\ub2e4.\n\n``recall()``: Mario\ub294 \uc790\uc2e0\uc758 \uae30\uc5b5\uc5d0\uc11c \ubb34\uc791\uc704\ub85c \uc77c\ub828\uc758 \uacbd\ud5d8\uc744 \uc0d8\ud50c\ub9c1\ud558\uc5ec\n\uac8c\uc784\uc744 \ud559\uc2b5\ud558\ub294 \ub370 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):  # \uc5f0\uc18d\uc131\uc744 \uc704\ud55c \ud558\uc704 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4.\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.memory = deque(maxlen=100000)\n        self.batch_size = 32\n\n    def cache(self, state, next_state, action, reward, done):\n        \"\"\"\n        Store the experience to self.memory (replay buffer)\n\n        \uc785\ub825\uac12:\n        state (LazyFrame),\n        next_state (LazyFrame),\n        action (int),\n        reward (float),\n        done (bool))\n        \"\"\"\n        state = state.__array__()\n        next_state = next_state.__array__()\n\n        if self.use_cuda:\n            state = torch.tensor(state).cuda()\n            next_state = torch.tensor(next_state).cuda()\n            action = torch.tensor([action]).cuda()\n            reward = torch.tensor([reward]).cuda()\n            done = torch.tensor([done]).cuda()\n        else:\n            state = torch.tensor(state)\n            next_state = torch.tensor(next_state)\n            action = torch.tensor([action])\n            reward = torch.tensor([reward])\n            done = torch.tensor([done])\n\n        self.memory.append((state, next_state, action, reward, done,))\n\n    def recall(self):\n        \"\"\"\n        \uba54\ubaa8\ub9ac\uc5d0\uc11c \uc77c\ub828\uc758 \uacbd\ud5d8\ub4e4\uc744 \uac80\uc0c9\ud569\ub2c8\ub2e4.\n        \"\"\"\n        batch = random.sample(self.memory, self.batch_size)\n        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud559\uc2b5\ud558\uae30(Learn)\n-----------------\n\n\ub9c8\ub9ac\uc624\ub294 `DDQN \uc54c\uace0\ub9ac\uc998 <https://arxiv.org/pdf/1509.06461>`__\n\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. DDQN \ub450\uac1c\uc758 ConvNets ( $Q_{online}$ \uacfc\n$Q_{target}$ ) \uc744 \uc0ac\uc6a9\ud558\uace0, \ub3c5\ub9bd\uc801\uc73c\ub85c \ucd5c\uc801\uc758 \ud589\ub3d9-\uac00\uce58 \ud568\uc218\uc5d0 \n\uadfc\uc0ac \uc2dc\ud0a4\ub824\uace0 \ud569\ub2c8\ub2e4.\n\n\uad6c\ud604\uc744 \ud560 \ub54c, \ud2b9\uc9d5 \uc0dd\uc131\uae30\uc5d0\uc11c ``\ud2b9\uc9d5\ub4e4`` \uc744 $Q_{online}$ \uc640 $Q_{target}$\n\uc5d0 \uacf5\uc720\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uac01\uac01\uc758 FC \ubd84\ub958\uae30\ub294\n\uac00\uc9c0\uace0 \uc788\ub3c4\ub85d \uc124\uacc4\ud569\ub2c8\ub2e4. $\\theta_{target}$ ($Q_{target}$\n\uc758 \ub9e4\uac1c\ubcc0\uc218 \uac12) \ub294 \uc5ed\uc804\ud30c\uc5d0 \uc758\ud574 \uac12\uc774 \uc5c5\ub370\uc774\ud2b8 \ub418\uc9c0 \uc54a\ub3c4\ub85d \uace0\uc815\ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\ub300\uc2e0, $\\theta_{online}$ \uc640 \uc8fc\uae30\uc801\uc73c\ub85c \ub3d9\uae30\ud654\ub97c \uc9c4\ud589\ud569\ub2c8\ub2e4. \n\uc774\uac83\uc5d0 \ub300\ud574\uc11c\ub294 \ucd94\ud6c4\uc5d0 \ub2e4\ub8e8\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.)\n\n\uc2e0\uacbd\ub9dd\n~~~~~~~~~~~~~~~~~~\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n    \"\"\"\uc791\uc740 cnn \uad6c\uc870\n  \uc785\ub825 -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> \ucd9c\ub825\n  \"\"\"\n\n    def __init__(self, input_dim, output_dim):\n        super().__init__()\n        c, h, w = input_dim\n\n        if h != 84:\n            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n        if w != 84:\n            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n\n        self.online = nn.Sequential(\n            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(3136, 512),\n            nn.ReLU(),\n            nn.Linear(512, output_dim),\n        )\n\n        self.target = copy.deepcopy(self.online)\n\n        # Q_target \ub9e4\uac1c\ubcc0\uc218 \uac12\uc740 \uace0\uc815\uc2dc\ud0b5\ub2c8\ub2e4.\n        for p in self.target.parameters():\n            p.requires_grad = False\n\n    def forward(self, input, model):\n        if model == \"online\":\n            return self.online(input)\n        elif model == \"target\":\n            return self.target(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TD \ucd94\uc815 & TD \ubaa9\ud45c\uac12\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\ud559\uc2b5\uc744 \ud558\ub294\ub370 \ub450 \uac00\uc9c0 \uac12\ub4e4\uc774 \ud3ec\ud568\ub429\ub2c8\ub2e4.\n\n**TD \ucd94\uc815** - \uc8fc\uc5b4\uc9c4 \uc0c1\ud0dc $s$ \uc5d0\uc11c \ucd5c\uc801\uc758 \uc608\uce21 $Q^*$. \n\n\\begin{align}{TD}_e = Q_{online}^*(s,a)\\end{align}\n\n**TD \ubaa9\ud45c** - \ud604\uc7ac\uc758 \ud3ec\uc0c1\uacfc \ub2e4\uc74c\uc0c1\ud0dc $s'$ \uc5d0\uc11c \ucd94\uc815\ub41c $Q^*$ \uc758 \ud569.\n\n\\begin{align}a' = argmax_{a} Q_{online}(s', a)\\end{align}\n\n\\begin{align}{TD}_t = r + \\gamma Q_{target}^*(s',a')\\end{align}\n\n\ub2e4\uc74c \ud589\ub3d9 $a'$ \uac00 \uc5b4\ub5a8\uc9c0 \ubaa8\ub974\uae30 \ub54c\ubb38\uc5d0 \n\ub2e4\uc74c \uc0c1\ud0dc $s'$ \uc5d0\uc11c $Q_{online}$ \uac12\uc774 \ucd5c\ub300\uac00 \ub418\ub3c4\ub85d \ud558\ub294\n\ud589\ub3d9 $a'$ \ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\uc5ec\uae30\uc5d0\uc11c \ubcc0\ud654\ub3c4 \uacc4\uc0b0\uc744 \ube44\ud65c\uc131\ud654\ud558\uae30 \uc704\ud574\n``td_target()`` \uc5d0\uc11c `@torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad>`__\n\ub370\ucf54\ub808\uc774\ud130(decorator)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n($\\theta_{target}$ \uc758 \uc5ed\uc804\ud30c \uacc4\uc0b0\uc774 \ud544\uc694\ub85c \ud558\uc9c0 \uc54a\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.gamma = 0.9\n\n    def td_estimate(self, state, action):\n        current_Q = self.net(state, model=\"online\")[\n            np.arange(0, self.batch_size), action\n        ]  # Q_online(s,a)\n        return current_Q\n\n    @torch.no_grad()\n    def td_target(self, reward, next_state, done):\n        next_state_Q = self.net(next_state, model=\"online\")\n        best_action = torch.argmax(next_state_Q, axis=1)\n        next_Q = self.net(next_state, model=\"target\")[\n            np.arange(0, self.batch_size), best_action\n        ]\n        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378\uc744 \uc5c5\ub370\uc774\ud2b8 \ud558\uae30.\n~~~~~~~~~~~~~~~~~~~~~~\n\n\ub9c8\ub9ac\uc624\uac00 \uc7ac\uc0dd \ubc84\ud37c\uc5d0\uc11c \uc785\ub825\uc744 \uc0d8\ud50c\ub9c1\ud560 \ub54c,  $TD_t$\n\uc640 $TD_e$ \ub97c \uacc4\uc0b0\ud569\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc774 \uc190\uc2e4\uc744 \uc774\uc6a9\ud558\uc5ec $Q_{online}$ \uc5ed\uc804\ud30c\ud558\uc5ec\n\ub9e4\uac1c\ubcc0\uc218 $\\theta_{online}$ \ub97c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4. ($\\alpha$ \ub294 \n``optimizer`` \uc5d0 \uc804\ub2ec\ub418\ub294 \ud559\uc2b5\ub960 ``lr`` \uc785\ub2c8\ub2e4.)\n\n\\begin{align}\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\end{align}\n\n$\\theta_{target}$ \uc740 \uc5ed\uc804\ud30c\ub97c \ud1b5\ud574 \uc5c5\ub370\uc774\ud2b8 \ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\ub300\uc2e0, \uc8fc\uae30\uc801\uc73c\ub85c $\\theta_{online}$ \uc758 \uac12\uc744 $\\theta_{target}$ \n\ub85c \ubcf5\uc0ac\ud569\ub2c8\ub2e4.\n\n\\begin{align}\\theta_{target} \\leftarrow \\theta_{online}\\end{align}\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n        self.loss_fn = torch.nn.SmoothL1Loss()\n\n    def update_Q_online(self, td_estimate, td_target):\n        loss = self.loss_fn(td_estimate, td_target)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        return loss.item()\n\n    def sync_Q_target(self):\n        self.net.target.load_state_dict(self.net.online.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uccb4\ud06c\ud3ec\uc778\ud2b8\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.\n~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n    def save(self):\n        save_path = (\n            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n        )\n        torch.save(\n            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n            save_path,\n        )\n        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub4e0 \uae30\ub2a5\uc744 \uc885\ud569\ud574\ubd05\uc2dc\ub2e4.\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n    def __init__(self, state_dim, action_dim, save_dir):\n        super().__init__(state_dim, action_dim, save_dir)\n        self.burnin = 1e4  # \ud559\uc2b5\uc744 \uc9c4\ud589\ud558\uae30 \uc804 \ucd5c\uc18c\ud55c\uc758 \uacbd\ud5d8\uac12.\n        self.learn_every = 3  # Q_online \uc5c5\ub370\uc774\ud2b8 \uc0ac\uc774\uc758 \uacbd\ud5d8 \ud69f\uc218.\n        self.sync_every = 1e4  # Q_target\uacfc Q_online sync \uc0ac\uc774\uc758 \uacbd\ud5d8 \uc218\n\n    def learn(self):\n        if self.curr_step % self.sync_every == 0:\n            self.sync_Q_target()\n\n        if self.curr_step % self.save_every == 0:\n            self.save()\n\n        if self.curr_step < self.burnin:\n            return None, None\n\n        if self.curr_step % self.learn_every != 0:\n            return None, None\n\n        # \uba54\ubaa8\ub9ac\ub85c\ubd80\ud130 \uc0d8\ud50c\ub9c1\uc744 \ud569\ub2c8\ub2e4.\n        state, next_state, action, reward, done = self.recall()\n\n        # TD \ucd94\uc815\uac12\uc744 \uac00\uc838\uc635\ub2c8\ub2e4.\n        td_est = self.td_estimate(state, action)\n\n        # TD \ubaa9\ud45c\uac12\uc744 \uac00\uc838\uc635\ub2c8\ub2e4.\n        td_tgt = self.td_target(reward, next_state, done)\n\n        # \uc2e4\uc2dc\uac04 Q(Q_online)\uc744 \ud1b5\ud574 \uc5ed\uc804\ud30c \uc190\uc2e4\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n        loss = self.update_Q_online(td_est, td_tgt)\n\n        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uae30\ub85d\ud558\uae30\n--------------\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport time, datetime\nimport matplotlib.pyplot as plt\n\n\nclass MetricLogger:\n    def __init__(self, save_dir):\n        self.save_log = save_dir / \"log\"\n        with open(self.save_log, \"w\") as f:\n            f.write(\n                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n            )\n        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n\n        # \uc9c0\ud45c(Metric)\uc640 \uad00\ub828\ub41c \ub9ac\uc2a4\ud2b8\uc785\ub2c8\ub2e4.\n        self.ep_rewards = []\n        self.ep_lengths = []\n        self.ep_avg_losses = []\n        self.ep_avg_qs = []\n\n        # \ubaa8\ub4e0 record() \ud568\uc218\ub97c \ud638\ucd9c\ud55c \ud6c4 \uc774\ub3d9 \ud3c9\uade0(Moving average)\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n        self.moving_avg_ep_rewards = []\n        self.moving_avg_ep_lengths = []\n        self.moving_avg_ep_avg_losses = []\n        self.moving_avg_ep_avg_qs = []\n\n        # \ud604\uc7ac \uc5d0\ud53c\uc2a4\ub4dc\uc5d0 \ub300\ud55c \uc9c0\ud45c\ub97c \uae30\ub85d\ud569\ub2c8\ub2e4.\n        self.init_episode()\n\n        # \uc2dc\uac04\uc5d0 \ub300\ud55c \uae30\ub85d\uc785\ub2c8\ub2e4.\n        self.record_time = time.time()\n\n    def log_step(self, reward, loss, q):\n        self.curr_ep_reward += reward\n        self.curr_ep_length += 1\n        if loss:\n            self.curr_ep_loss += loss\n            self.curr_ep_q += q\n            self.curr_ep_loss_length += 1\n\n    def log_episode(self):\n        \"\uc5d0\ud53c\uc2a4\ub4dc\uc758 \ub05d\uc744 \ud45c\uc2dc\ud569\ub2c8\ub2e4.\"\n        self.ep_rewards.append(self.curr_ep_reward)\n        self.ep_lengths.append(self.curr_ep_length)\n        if self.curr_ep_loss_length == 0:\n            ep_avg_loss = 0\n            ep_avg_q = 0\n        else:\n            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n        self.ep_avg_losses.append(ep_avg_loss)\n        self.ep_avg_qs.append(ep_avg_q)\n\n        self.init_episode()\n\n    def init_episode(self):\n        self.curr_ep_reward = 0.0\n        self.curr_ep_length = 0\n        self.curr_ep_loss = 0.0\n        self.curr_ep_q = 0.0\n        self.curr_ep_loss_length = 0\n\n    def record(self, episode, epsilon, step):\n        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n        self.moving_avg_ep_rewards.append(mean_ep_reward)\n        self.moving_avg_ep_lengths.append(mean_ep_length)\n        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n\n        last_record_time = self.record_time\n        self.record_time = time.time()\n        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n\n        print(\n            f\"Episode {episode} - \"\n            f\"Step {step} - \"\n            f\"Epsilon {epsilon} - \"\n            f\"Mean Reward {mean_ep_reward} - \"\n            f\"Mean Length {mean_ep_length} - \"\n            f\"Mean Loss {mean_ep_loss} - \"\n            f\"Mean Q Value {mean_ep_q} - \"\n            f\"Time Delta {time_since_last_record} - \"\n            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n        )\n\n        with open(self.save_log, \"a\") as f:\n            f.write(\n                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n                f\"{time_since_last_record:15.3f}\"\n                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n            )\n\n        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n            plt.savefig(getattr(self, f\"{metric}_plot\"))\n            plt.clf()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uac8c\uc784\uc744 \uc2e4\ud589\uc2dc\ucf1c\ubd05\uc2dc\ub2e4!\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n\uc774\ubc88 \uc608\uc81c\uc5d0\uc11c\ub294 10\uac1c\uc758 \uc5d0\ud53c\uc18c\ub4dc\uc5d0 \ub300\ud574 \ud559\uc2b5 \ub8e8\ud504\ub97c \uc2e4\ud589\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.\ud558\uc9c0\ub9cc \ub9c8\ub9ac\uc624\uac00 \uc9c4\uc815\uc73c\ub85c \n\uc138\uacc4\ub97c \ud559\uc2b5\ud558\uae30 \uc704\ud574\uc11c\ub294 \uc801\uc5b4\ub3c4 40000\uac1c\uc758 \uc5d0\ud53c\uc18c\ub4dc\uc5d0 \ub300\ud574 \ud559\uc2b5\uc744 \uc2dc\ud0ac \uac83\uc744 \uc81c\uc548\ud569\ub2c8\ub2e4!\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "use_cuda = torch.cuda.is_available()\nprint(f\"Using CUDA: {use_cuda}\")\nprint()\n\nsave_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\nsave_dir.mkdir(parents=True)\n\nmario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n\nlogger = MetricLogger(save_dir)\n\nepisodes = 10\nfor e in range(episodes):\n\n    state = env.reset()\n\n    # \uac8c\uc784\uc744 \uc2e4\ud589\uc2dc\ucf1c\ubd05\uc2dc\ub2e4!\n    while True:\n\n        # \ud604\uc7ac \uc0c1\ud0dc\uc5d0\uc11c \uc5d0\uc774\uc804\ud2b8 \uc2e4\ud589\ud558\uae30\n        action = mario.act(state)\n\n        # \uc5d0\uc774\uc804\ud2b8\uac00 \uc561\uc158 \uc218\ud589\ud558\uae30\n        next_state, reward, done, info = env.step(action)\n\n        # \uae30\uc5b5\ud558\uae30\n        mario.cache(state, next_state, action, reward, done)\n\n        # \ubc30\uc6b0\uae30\n        q, loss = mario.learn()\n\n        # \uae30\ub85d\ud558\uae30\n        logger.log_step(reward, loss, q)\n\n        # \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\ud558\uae30\n        state = next_state\n\n        # \uac8c\uc784\uc774 \ub05d\ub0ac\ub294\uc9c0 \ud655\uc778\ud558\uae30\n        if done or info[\"flag_get\"]:\n            break\n\n    logger.log_episode()\n\n    if e % 20 == 0:\n        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uacb0\ub860\n\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294 PyTorch\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac8c\uc784 \ud50c\ub808\uc774 AI\ub97c \ud6c8\ub828\ud558\ub294 \ubc29\ubc95\uc744 \uc0b4\ud3b4\ubcf4\uc558\uc2b5\ub2c8\ub2e4. `OpenAI gym <https://gym.openai.com/>`__\n\uc5d0 \uc788\ub294 \uc5b4\ub5a4 \uac8c\uc784\uc774\ub4e0 \ub3d9\uc77c\ud55c \ubc29\ubc95\uc73c\ub85c AI\ub97c \ud6c8\ub828\uc2dc\ud0a4\uace0 \uac8c\uc784\uc744 \uc9c4\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc774 \ub3c4\uc6c0\uc774 \ub418\uc5c8\uae30\ub97c \ubc14\ub77c\uba70, \n`Github \uc800\uc7a5\uc18c <https://github.com/yuansongFeng/MadMario/>`__ \uc5d0\uc11c \ud3b8\ud558\uac8c \uc800\uc790\ub4e4\uc5d0\uac8c \uc5f0\ub77d\uc744 \ud558\uc154\ub3c4 \ub429\ub2c8\ub2e4!\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}