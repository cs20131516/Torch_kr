{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPyTorch\ub97c \uc774\uc6a9\ud558\uc5ec \ub274\ub7f4 \ubcc0\ud658(Neural Transfer)\n=============================\n\n\n**Author**: `Alexis Jacq <https://alexis-jacq.github.io>`_\n\n**Edited by**: `Winston Herring <https://github.com/winston6>`_\n\n**\ubc88\uc5ed**: `\uc815\uc7ac\ubbfc <https://github.com/jjeamin>`_\n\n\uc18c\uac1c\n------------\n\n\uc774\ubc88 \ud29c\ud1a0\ub9ac\uc5bc\uc740 Leon A. Gatys, Alexander S. Ecker and Matthias Bethge\uc5d0 \uc758\ud574 \uac1c\ubc1c\ub41c\n`\ub274\ub7f4 \uc2a4\ud0c0\uc77c(Neural-Style) \uc54c\uace0\ub9ac\uc998 <https://arxiv.org/abs/1508.06576>`__ \uc744 \uad6c\ud604\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud558\uc5ec \uc124\uba85\ud569\ub2c8\ub2e4.\n\ub274\ub7f4 \uc2a4\ud0c0\uc77c(Neural-Style), \ub610\ub294 \ub274\ub7f4 \ubcc0\ud658(Neural-Transfer)\uc744 \uc0ac\uc6a9\ud558\uba74 \uc774\ubbf8\uc9c0\ub97c \uc0c8\ub85c\uc6b4 \uc608\uc220\uc801 \uc2a4\ud0c0\uc77c\ub85c \uc7ac\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774 \uc54c\uace0\ub9ac\uc998\uc740 \uc785\ub825 \uc774\ubbf8\uc9c0, Content \uc774\ubbf8\uc9c0, Style \uc774\ubbf8\uc9c0 3\uac1c\uc758 \uc774\ubbf8\uc9c0\ub97c \uac00\uc838\uc640\uc11c Content \uc774\ubbf8\uc9c0\uc758 Content\uc640 Style \uc774\ubbf8\uc9c0\uc758 \uc608\uc220\uc801 Style\uc744 \ub2ee\ub3c4\ub85d \uc785\ub825\uc744 \ubcc0\ud658\ud569\ub2c8\ub2e4.\n\n.. figure:: /_static/img/neural-style/neuralstyle.png\n   :alt: content1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uae30\ubcf8 \uc6d0\ub9ac\n--------------------\n\n\uc6d0\ub9ac\ub294 \uac04\ub2e8\ud569\ub2c8\ub2e4.\nContent($D_C$)\uc640 Style($D_S$)\uc5d0 \ub300\ud574 \ud558\ub098\uc529 \ub450 \uac1c\uc758 \uac70\ub9ac\ub97c \uc815\uc758\ud569\ub2c8\ub2e4.\u00a0\n$D_C$ \ub294 \ub450 \uc774\ubbf8\uc9c0 \uac04\uc758 Content\uc758 \ucc28\uc774\ub97c \uce21\uc815\ud558\uace0 $D_S$ \ub294 \ub450 \uc774\ubbf8\uc9c0 \uac04\uc758 Style\uc758 \ucc28\uc774\ub97c \uce21\uc815\ud569\ub2c8\ub2e4.\n\uadf8\ub7f0 \ub2e4\uc74c, \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ub418\ub294 \uc138 \ubc88\uc9f8 \uc774\ubbf8\uc9c0\ub97c \uac00\uc838\uc640\uc11c\nContent \uc774\ubbf8\uc9c0\ub85c Content \uac70\ub9ac\uc640 Style \uc774\ubbf8\uc9c0\ub85c Style \uac70\ub9ac\ub97c \ucd5c\uc18c\ud654\ud558\ub3c4\ub85d \ubcc0\ud658\ud569\ub2c8\ub2e4.\n\uc774\uc81c \ud544\uc694\ud55c \ud328\ud0a4\uc9c0\ub97c \uac00\uc838\uc640\uc11c \ub274\ub7f4 \ubcc0\ud658(neural transfer)\uc744 \uc2dc\uc791\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\ud328\ud0a4\uc9c0 \ubd88\ub7ec\uc624\uae30 \ubc0f \uc7a5\uce58 \uc120\ud0dd\n-----------------------------------------\n\ub2e4\uc74c\uc740 \ub274\ub7f4 \ubcc0\ud658(neural transfer)\uc744 \uad6c\ud604\ud558\ub294\ub370 \ud544\uc694\ud55c \ud328\ud0a4\uc9c0 \ubaa9\ub85d\uc785\ub2c8\ub2e4.\n\n-  ``torch``, ``torch.nn``, ``numpy`` (PyTorch\ub85c \uc2e0\uacbd\ub9dd\uc744 \uad6c\ud604\ud558\uae30 \uc704\ud55c \ud544\uc218 \ud328\ud0a4\uc9c0)\n-  ``torch.optim`` (\ud6a8\uc728\uc801\uc778 \uacbd\uc0ac \ud558\uac15\ubc95)\n-  ``PIL``, ``PIL.Image``, ``matplotlib.pyplot`` (\uc774\ubbf8\uc9c0 \ubd88\ub7ec\uc624\uae30 \ubc0f \uc774\ubbf8\uc9c0 \ud45c\uc2dc)\n-  ``torchvision.transforms`` (PIL \uc774\ubbf8\uc9c0\ub97c \ud150\uc11c\ub85c \ubcc0\ud658)\n-  ``torchvision.models`` (\ubbf8\ub9ac \ud559\uc2b5\ub41c \ubaa8\ub378 \ubd88\ub7ec\uc624\uae30 \ubc0f \ud559\uc2b5)\n-  ``copy`` (\ubaa8\ub378\uc744 \ubcf5\uc0ac; \uc2dc\uc2a4\ud15c \ud328\ud0a4\uc9c0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub2e4\uc74c\uc73c\ub85c, \ub124\ud2b8\uc6cc\ud06c\ub97c \uc2e4\ud589 \ud560 \uc7a5\uce58\ub97c \uc120\ud0dd\ud558\uace0 Style \uc774\ubbf8\uc9c0\uc640 Content \uc774\ubbf8\uc9c0\ub97c \uac00\uc838\uc640\uc57c \ud569\ub2c8\ub2e4.\n\ud070 \uc774\ubbf8\uc9c0\ub85c \ub274\ub7f4 \ubcc0\ud658(neural transfer) \uc54c\uace0\ub9ac\uc998\uc744 \uc2e4\ud589\ud558\uba74 \uc2dc\uac04\uc774 \uc624\ub798 \uac78\ub9ac\uace0\nGPU\uc5d0\uc11c \uc2e4\ud589\ud560 \ub54c \ud6e8\uc52c \ube68\ub77c\uc9d1\ub2c8\ub2e4.\n``torch.cuda.is_available()`` \ub97c \uc0ac\uc6a9\ud558\uc5ec GPU\ub97c \uc774\uc6a9\ud560 \uc218 \uc788\ub294\uc9c0 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ub2e4\uc74c\uc73c\ub85c, \ud29c\ud1a0\ub9ac\uc5bc \uc804\uccb4\uc5d0\uc11c \uc0ac\uc6a9 \ud560 ``torch.device`` \ub97c \uc124\uc815\ud569\ub2c8\ub2e4.\n\ub610\ud55c ``.to(device)`` \uba54\uc18c\ub4dc\ub294 \ud150\uc11c \ub610\ub294 \ubaa8\ub4c8\uc744 \uc6d0\ud558\ub294 \uc7a5\uce58\ub85c \uc774\ub3d9\ud558\ub294\ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\ubbf8\uc9c0 \ubd88\ub7ec\uc624\uae30\n------------------\n\n\uc9c0\uae08 Style, Content \uc774\ubbf8\uc9c0\ub97c \uac00\uc838\uc62c \uac83\uc785\ub2c8\ub2e4. \uc6d0\ubcf8 PIL \uc774\ubbf8\uc9c0\ub294 0\uacfc 255 \uc0ac\uc774\uc758 \uac12\uc744 \uac16\uc9c0\ub9cc,\n\ud150\uc11c\ub85c \ubcc0\ud658\ub420 \ub54c 0\uc5d0\uc11c 1\uc0ac\uc774\ub85c \ubcc0\ud658\ub429\ub2c8\ub2e4.\n\uc774\ubbf8\uc9c0\ub3c4 \ub3d9\uc77c\ud55c \ucc28\uc6d0\uc744 \uac00\uc9c0\ub3c4\ub85d \ud06c\uae30\uac00 \ubcc0\ud658\ub418\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n\uc8fc\ubaa9\ud574\uc57c \ud560 \uc911\uc694\ud55c \uc138\ubd80\uc0ac\ud56d\uc740 torch \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \uc2e0\uacbd\ub9dd\uc740 0\uc5d0\uc11c 1\uc0ac\uc774\uc758 \ud150\uc11c \uac12\uc73c\ub85c \ud6c8\ub828\ub41c\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\ub9cc\uc57d 0\uc5d0\uc11c 255\uac12\uc744 \uac00\uc9c0\ub294 \ud150\uc11c \uc774\ubbf8\uc9c0\uac00 \ub124\ud2b8\uc6cc\ud06c\uc5d0 \uc785\ub825\ub418\ub294 \uacbd\uc6b0,\n\ud65c\uc131\ud654 \ub41c \ud2b9\uc9d5 \ub9f5\uc774 Content\uc640 Style\uc744 \uac10\uc9c0\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\n\uadf8\ub7ec\ub098, Caffe \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \ubbf8\ub9ac \ud559\uc2b5\ub41c \ub124\ud2b8\uc6cc\ud06c\ub294 0\uc5d0\uc11c 255\uac12\uc744 \uac00\uc9c0\ub294 \uc785\ub825\uc73c\ub85c \ud6c8\ub828\ub429\ub2c8\ub2e4.\n\n\n.. Note::\n    \ud29c\ud1a0\ub9ac\uc5bc\uc744 \uc2e4\ud589\ud558\ub294\ub370 \ud544\uc694\ud55c \uc774\ubbf8\uc9c0\ub97c \ub2e4\uc6b4\ub85c\ub4dc \ud560 \uc218 \uc788\ub294 \uc8fc\uc18c\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n    `picasso.jpg <https://tutorials.pytorch.kr/_static/img/neural-style/picasso.jpg>`__ \uc640\n    `dancing.jpg <https://tutorials.pytorch.kr/_static/img/neural-style/dancing.jpg>`__.\n    \ub450 \uc774\ubbf8\uc9c0\ub97c \ub2e4\uc6b4\ub85c\ub4dc\ud558\uace0 \ud604\uc7ac \uc791\uc5c5 \ud3f4\ub354\uc758 ``images`` \ud3f4\ub354\uc5d0 \ucd94\uac00\ud558\uc138\uc694.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# \uc6d0\ud558\ub294 \ucd9c\ub825 \uc774\ubbf8\uc9c0\uc758 \ud06c\uae30\nimsize = 512 if torch.cuda.is_available() else 128  # \ub9cc\uc57d GPU\uac00 \uc5c6\uc744 \uacbd\uc6b0 \uc791\uc740 \ud06c\uae30\ub97c \uc0ac\uc6a9\n\nloader = transforms.Compose([\n    transforms.Resize(imsize),  # \uac00\uc838\uc628 \uc774\ubbf8\uc9c0 \ud06c\uae30 \uc870\uc815\n    transforms.ToTensor()])  # \ud150\uc11c\ub85c \ubcc0\ud658\n\n\ndef image_loader(image_name):\n    image = Image.open(image_name)\n    # \ub124\ud2b8\uc6cc\ud06c\uc758 \uc785\ub825 \ucc28\uc6d0\uc5d0 \ub9de\ucd94\uae30 \uc704\ud574 \ud544\uc694\ud55c \uac00\uc9dc \ubc30\uce58 \ucc28\uc6d0\n    image = loader(image).unsqueeze(0)\n    return image.to(device, torch.float)\n\n\nstyle_img = image_loader(\"./data/images/neural-style/picasso.jpg\")\ncontent_img = image_loader(\"./data/images/neural-style/dancing.jpg\")\n\nassert style_img.size() == content_img.size(), \\\n    \"we need to import style and content images of the same size\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c \uc774\ubbf8\uc9c0\ub97c PIL \ud615\uc2dd\uc73c\ub85c \ub2e4\uc2dc \ubcc0\ud658\ud558\uace0 ``plt.imshow`` \ub97c\n\uc0ac\uc6a9\ud574 \uc774\ubbf8\uc9c0\ub97c \ud45c\uc2dc\ud558\ub294 \ud568\uc218\ub97c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4.\nContent\uc640 Style \uc774\ubbf8\uc9c0\ub97c \ud45c\uc2dc\ud558\uc5ec \uc62c\ubc14\ub974\uac8c \uac00\uc838\uc654\ub294\uc9c0 \ud655\uc778\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unloader = transforms.ToPILImage()  # PIL \uc774\ubbf8\uc9c0\ub85c \ub2e4\uc2dc \ubcc0\ud658\n\nplt.ion()\n\ndef imshow(tensor, title=None):\n    image = tensor.cpu().clone()  # \ud150\uc11c\ub97c \ubcf5\uc81c\ud558\uc5ec \ubcc0\uacbd\ud558\uc9c0 \uc54a\uc74c\n    image = image.squeeze(0)      # \uac00\uc9dc \ubc30\uce58 \ucc28\uc6d0 \uc81c\uac70\n    image = unloader(image)\n    plt.imshow(image)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001) # plots\uac00 \uc5c5\ub370\uc774\ud2b8 \ub418\ub3c4\ub85d \uc7a0\uc2dc \uba48\ucda4\n\n\nplt.figure()\nimshow(style_img, title='Style Image')\n\nplt.figure()\nimshow(content_img, title='Content Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc190\uc2e4 \ud568\uc218\n--------------\n\ucf58\ud150\uce20 \uc190\uc2e4(Content Loss)\n~~~~~~~~~~~~\n\nContent \uc190\uc2e4\uc740 \uac01 \uacc4\uce35\uc5d0 \ub300\ud55c Content \uac70\ub9ac\uc758 \uac00\uc911\uce58 \ubc84\uc804\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\uc774 \ud568\uc218\ub294 \uc785\ub825 $X$ \ub97c \ucc98\ub9ac\ud558\ub294 \ub808\uc774\uc5b4 $L$ \uc758 \ud2b9\uc9d5 \ub9f5 $F_{XL}$ \uc744 \uac00\uc838\uc640\uc11c\n\uc774\ubbf8\uc9c0 $X$ \uc640 Content \uc774\ubbf8\uc9c0 $C$ \uc0ac\uc774\uc758\n\uac00\uc911 \ucf58\ud150\uce20 \uac70\ub9ac(weighted content distance) $w_{CL}.D_C^L(X,C)$ \ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.\nContent \uac70\ub9ac\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 Content \uc774\ubbf8\uc9c0($F_{CL}$)\uc758 \ud2b9\uc9d5 \ub9f5\uc744 \ud568\uc218\uc5d0\uc11c \uc54c\uace0 \uc788\uc5b4\uc57c\ud569\ub2c8\ub2e4.\n$F_{CL}$ \uc744 \uc785\ub825\uc73c\ub85c \uc0ac\uc6a9\ud558\ub294 \uc0dd\uc131\uc790\ub97c \uc0ac\uc6a9\ud574 \uc774 \ud568\uc218\ub97c torch \ubaa8\ub4c8\ub85c \uad6c\ud604\ud569\ub2c8\ub2e4.\n\uac70\ub9ac $\\|F_{XL} - F_{CL}\\|^2$ \ub294 \ub450 \uac1c\uc758 \ud2b9\uc9d5 \ub9f5 \uc9d1\ud569\uc758 \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28\uc774\uba70 ``nn.MSELoss`` \ub97c \uc0ac\uc6a9\ud558\uc5ec \uacc4\uc0b0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\nContent \uac70\ub9ac\ub97c \uacc4\uc0b0\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub418\ub294 \ud569\uc131\uacf1 \uacc4\uce35 \ubc14\ub85c \ub4a4\uc5d0 Content \uc190\uc2e4 \ubaa8\ub4c8\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4.\n\uc774\ub807\uac8c \ud558\uba74 \uc785\ub825 \uc774\ubbf8\uc9c0\uac00 \uc785\ub825\ub420 \ub54c\ub9c8\ub2e4 Content \uc190\uc2e4\uc774 \uc6d0\ud558\ub294 \ub808\uc774\uc5b4\uc5d0\uc11c\n\uacc4\uc0b0\ub418\uace0 autograd \uc744 \ud1b5\ud574 \ubaa8\ub4e0 \uae30\uc6b8\uae30\uac00 \uacc4\uc0b0\ub429\ub2c8\ub2e4.\n\uc774\uc81c Content \uc190\uc2e4 \uacc4\uce35\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 Content \uc190\uc2e4\uc744 \uacc4\uc0b0\ud55c \ub2e4\uc74c \uacc4\uce35\uc758 \uc785\ub825\uc744 \ubc18\ud658\ud558\ub294\n``forward`` \uba54\uc18c\ub4dc\ub97c \uc815\uc758\ud574\uc57c\ud569\ub2c8\ub2e4.\n\uacc4\uc0b0\ub41c \uc190\uc2e4\uc740 \ubaa8\ub4c8\uc758 \ub9e4\uac1c \ubcc0\uc218\ub85c\uc368 \uc800\uc7a5\ub429\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class ContentLoss(nn.Module):\n\n    def __init__(self, target,):\n        super(ContentLoss, self).__init__()\n        # \uae30\uc6b8\uae30\ub97c \ub3d9\uc801\uc73c\ub85c \uacc4\uc0b0\ud558\ub294\ub370 \uc0ac\uc6a9\ub418\ub294 tree\ub85c\ubd80\ud130 \ud0c0\uae43 Content\ub97c `\ubd84\ub9ac(detach)` \ud569\ub2c8\ub2e4.\n        # \uc774\uac83\uc740 \ubcc0\uc218\uac00 \uc544\ub2c8\ub77c \uba85\uc2dc\ub41c \uac12\uc785\ub2c8\ub2e4.\n        # \uadf8\ub807\uc9c0 \uc54a\uc73c\uba74 criterion\uc758 forward \uba54\uc18c\ub4dc\uc5d0\uc11c \uc624\ub958\uac00 \ubc1c\uc0dd\ud569\ub2c8\ub2e4.\n        self.target = target.detach()\n\n    def forward(self, input):\n        self.loss = F.mse_loss(input, self.target)\n        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. Note::\n   **\uc911\uc694 \uc138\ubd80 \uc0ac\ud56d**: \ubaa8\ub4c8\uc758 \uc774\ub984\uc740 ``ContentLoss`` \uc9c0\ub9cc, \uc2e4\uc81c PyTorch\uc758 \uc190\uc2e4 \ud568\uc218\ub294 \uc544\ub2d9\ub2c8\ub2e4.\n   \ub9cc\uc57d Content \uc190\uc2e4\uc744 PyTorch\uc758 \uc190\uc2e4 \ud568\uc218\ub85c \uc815\uc758\ud558\ub824\uba74 Pytorch\uc758 autograd \ud568\uc218\ub97c \uc0dd\uc131\ud558\uc5ec\n   ``backward`` \uba54\uc18c\ub4dc\uc5d0\uc11c \uc218\ub3d9\uc73c\ub85c \uae30\uc6b8\uae30\ub97c \ub2e4\uc2dc \uacc4\uc0b0 \ubc0f \uad6c\ud604 \ud574\uc57c \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc2a4\ud0c0\uc77c \uc190\uc2e4(Style Loss)\n~~~~~~~~~~\n\nStyle \uc190\uc2e4 \ubaa8\ub4c8\uc740 Content \uc190\uc2e4 \ubaa8\ub4c8\uacfc \uc720\uc0ac\ud558\uac8c \uad6c\ud604\ub429\ub2c8\ub2e4.\n\ub124\ud2b8\uc6cc\ud06c\uc5d0\uc11c \ud574\ub2f9 \uacc4\uce35\uc758 Style \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\ub294 \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4.\nStyle \uc190\uc2e4\uc744 \uacc4\uc0b0\ud558\uae30 \uc704\ud574\uc11c, gram \ud589\ub82c $G_{XL}$ \ub97c \uacc4\uc0b0\ud574\uc57c\ud569\ub2c8\ub2e4.\ngram \ud589\ub82c\uc740 \uc8fc\uc5b4\uc9c4 \ud589\ub82c\uc5d0 \uc804\uce58 \ud589\ub82c\uc744 \uacf1\ud55c \uacb0\uacfc\uc785\ub2c8\ub2e4.\n\uc774 \uc5b4\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \uc8fc\uc5b4\uc9c4 \ud589\ub82c\uc740 \uacc4\uce35 $L$ \uc758 \ud2b9\uc9d5 \ub9f5 $F_{XL}$ \uc758 \uc7ac\uad6c\uc131\ub41c \ubc84\uc804\uc785\ub2c8\ub2e4.\n$F_{XL}$ \ub294 $K$\\ x\\ $N$ \ud589\ub82c\uc778 $\\hat{F}_{XL}$ \uc744 \ud615\uc131\ud558\ub3c4\ub85d \uc7ac\uad6c\uc131\ub418\uba70,\n$K$ \ub294 \uacc4\uce35 $L$ \uc758 \ud2b9\uc9d5 \ub9f5\uc758 \uc218\uc774\uace0 $N$\ub294 \ubca1\ud130\ud654\ub41c \ud2b9\uc9d5 \ub9f5\uc758 \uae38\uc774\uc785\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uc5b4, $\\hat{F}_{XL}$ \uc758 \uccab \ubc88\uc9f8 \uc904\uc740 \ubca1\ud130\ud654\ub41c \ud2b9\uc9d5 \ub9f5 $F_{XL}^1$ \uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4.\n\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c, gram \ud589\ub82c\uc740 \ud589\ub82c\uc5d0\uc11c \uac01 \uac12\uc744 \uac12\uc758 \ucd1d \uac1c\uc218\ub85c \ub098\ub204\uc5b4 \uc815\uaddc\ud654 \ud574\uc57c \ud569\ub2c8\ub2e4.\n\uc815\uaddc\ud654\ub294 \ud070 $N$ \ucc28\uc6d0\uc744 \uac00\uc9c0\ub294 $\\hat{F}_{XL}$ \ud589\ub82c\uc774 gram \ud589\ub82c\uc5d0\uc11c \ub354 \ud070 \uac12\uc744 \uc0dd\uc131\ud55c\ub2e4\ub294 \uc0ac\uc2e4\uc5d0 \ub300\uc751\ud558\uae30 \uc704\ud55c \uac83\uc785\ub2c8\ub2e4.\n\n\uc774\uc640 \uac19\uc740 \ub354 \ud070 \uac12\uc740 \uccab \ubc88\uc9f8 \uacc4\uce35(\ud480\ub9c1 \uacc4\uce35 \uc804)\uc774 \uacbd\uc0ac \ud558\uac15 \uc911\uc5d0 \ub354 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce58\uac8c\ud569\ub2c8\ub2e4.\n\nStyle \ud2b9\uc9d5\uc740 \ub124\ud2b8\uc6cc\ud06c\uc758 \uae4a\uc740 \uacc4\uce35\uc5d0 \uc788\ub294 \uacbd\ud5a5\uc774 \uc788\uae30\uc5d0 \uc815\uaddc\ud654 \ub2e8\uacc4\uac00 \uc911\uc694\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input):\n    a, b, c, d = input.size()  # a=\ubc30\uce58 \ud06c\uae30(=1)\n    # b=\ud2b9\uc9d5 \ub9f5\uc758 \uc218\n    # (c,d)=\ud2b9\uc9d5 \ub9f5\uc758 \ucc28\uc6d0 (N=c*d)\n\n    features = input.view(a * b, c * d)  # F_XL\uc744 \\hat F_XL\ub85c \ud06c\uae30 \uc870\uc815\n\n    G = torch.mm(features, features.t())  # gram product\ub97c \uacc4\uc0b0\n\n    # \uac01 \ud2b9\uc9d5 \ub9f5\uc774 \uac16\ub294 \uac12\uc758 \uc218\ub85c \ub098\ub204\uc5b4\n    # gram \ud589\ub82c\uc758 \uac12\uc744 '\uc815\uaddc\ud654'\n    return G.div(a * b * c * d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\uc81c Style \uc190\uc2e4 \ubaa8\ub4c8\uc740 Content \uc190\uc2e4 \ubaa8\ub4c8\uacfc \uac70\uc758 \ub611\uac19\uc544 \ubcf4\uc785\ub2c8\ub2e4.\nStyle \uac70\ub9ac\ub294 $G_{XL}$ \uc640 $G_{SL}$ \uc0ac\uc774\uc758 \ud3c9\uade0 \uc81c\uacf1 \uc624\ucc28\ub97c \uc0ac\uc6a9\ud574 \uacc4\uc0b0\ub429\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class StyleLoss(nn.Module):\n\n    def __init__(self, target_feature):\n        super(StyleLoss, self).__init__()\n        self.target = gram_matrix(target_feature).detach()\n\n    def forward(self, input):\n        G = gram_matrix(input)\n        self.loss = F.mse_loss(G, self.target)\n        return input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \uac00\uc838\uc624\uae30\n-------------------\n\n\uc774\uc81c\ub294 \uc0ac\uc804 \ud6c8\ub828\ub41c \uc2e0\uacbd\ub9dd\uc744 \uac00\uc838\uc640\uc57c \ud569\ub2c8\ub2e4.\n\ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c \uac83\uacfc \uac19\uc774 19 \uacc4\uce35\uc744 \uac00\uc9c4 VGG \ub124\ud2b8\uc6cc\ud06c\ub97c \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\nPyTorch\uc758 VGG \uad6c\ud604\uc740 ``features`` (\ud569\uc131\uacf1 \ubc0f \ud480\ub9c1 \uacc4\uce35 \ud3ec\ud568) \uc640\n``classifier`` (\uc644\uc804 \uc5f0\uacb0 \uacc4\uce35 \ud3ec\ud568) \ub450 \uac00\uc9c0 \ud558\uc704 ``Sequential`` \ubaa8\ub4c8\ub85c \uad6c\ubd84 \ub41c \ubaa8\ub4c8\uc785\ub2c8\ub2e4.\nContent \ubc0f Style \uc190\uc2e4\uc744 \uce21\uc815\ud558\uae30 \uc704\ud574\uc11c \uac01 \ud569\uc131\uacf1 \uacc4\uce35\uc758 \ucd9c\ub825\uc774 \ud544\uc694\ud558\uae30 \ub54c\ubb38\uc5d0 ``features`` \ubaa8\ub4c8\uc744 \uc0ac\uc6a9\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc77c\ubd80 \uacc4\uce35\uc740 \ud6c8\ub828\ud558\ub294 \uc911 \ud3c9\uac00\uc640 \ub2e4\ub978 \ub3d9\uc791\uc744 \ud558\ubbc0\ub85c, \ub124\ud2b8\uc6cc\ud06c\ub97c ``.eval()`` \ub97c \uc0ac\uc6a9\ud574 \ud3c9\uac00 \ubaa8\ub4dc\ub85c \uc124\uc815\ud574\uc57c\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ucd94\uac00\uc801\uc73c\ub85c, VGG \ub124\ud2b8\uc6cc\ud06c\ub294\nmean=[0.485, 0.456, 0.406] \uc640 std=[0.229, 0.224, 0.225]\ub85c \uac01 \ucc44\ub110\uc774 \uc815\uaddc\ud654 \ub41c\n\uc774\ubbf8\uc9c0\ub85c \ud6c8\ub828\ub429\ub2c8\ub2e4.\n\uc774\ubbf8\uc9c0\ub97c \ub124\ud2b8\uc6cc\ud06c\ub85c \uc785\ub825\ud558\uae30 \uc804\uc5d0 \uc815\uaddc\ud654\ud558\ub294\ub370 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\ncnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n\n# \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \uc815\uaddc\ud654\ud558\ub294 \ubaa8\ub4c8\uc744 \uc0dd\uc131\ud558\uc5ec \uc27d\uac8c nn.Sequential\uc5d0 \ub123\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\nclass Normalization(nn.Module):\n    def __init__(self, mean, std):\n        super(Normalization, self).__init__()\n        # .view \ub294 mean\uacfc std\uc744 \ud655\uc778\ud574 [B x C x H x W] \ud615\ud0dc\uc758\n        # \uc774\ubbf8\uc9c0 \ud150\uc11c\ub97c \uc9c1\uc811\uc801\uc73c\ub85c \uc791\uc5c5\ud560 \uc218 \uc788\ub3c4\ub85d [C x 1 x 1] \ud615\ud0dc\ub85c \ub9cc\ub4ed\ub2c8\ub2e4.\n        # B\ub294 \ubc30\uce58 \ud06c\uae30\uc785\ub2c8\ub2e4. C\ub294 \ucc44\ub110\uc758 \uc218\uc785\ub2c8\ub2e4. H\ub294 \ub192\uc774\uace0 W\ub294 \ub108\ube44\uc785\ub2c8\ub2e4.\n        self.mean = torch.tensor(mean).view(-1, 1, 1)\n        self.std = torch.tensor(std).view(-1, 1, 1)\n\n    def forward(self, img):\n        # img \uc815\uaddc\ud654\n        return (img - self.mean) / self.std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``Sequential`` \ubaa8\ub4c8\uc740 \uc21c\uc11c\uac00 \uc788\ub294 \ud558\uc704 \ubaa8\ub4c8\uc758 \ub9ac\uc2a4\ud2b8\uac00 \ud3ec\ud568\ub429\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uc5b4, ``vgg19.features`` \uc740 \uc62c\ubc14\ub978 \uc21c\uc11c\ub85c \uc815\ub82c \ub41c\n\uc2dc\ud000\uc2a4(Conv2d, ReLU, MaxPool2d, Conv2d, ReLU\u2026)\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\nContent \uc190\uc2e4\uacfc Style \uc190\uc2e4 \uacc4\uce35\uc744 \uac10\uc9c0\ud558\ub294 \ud569\uc131\uacf1 \uacc4\uce35 \ubc14\ub85c \ub4a4\uc5d0 \ucd94\uac00\ud574\uc57c\ud569\ub2c8\ub2e4.\n\uc774\ub807\uac8c \ud558\uae30 \uc704\ud574\uc11c\ub294 Content \uc190\uc2e4\uacfc Style \uc190\uc2e4 \ubaa8\ub4c8\uc774\n\uc62c\ubc14\ub974\uac8c \uc0bd\uc785\ub41c \uc0c8\ub85c\uc6b4 ``Sequential`` \ubaa8\ub4c8\uc744 \ub9cc\ub4e4\uc5b4\uc57c \ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Style / Content \uc190\uc2e4 \uacc4\uc0b0\uc744 \uc6d0\ud558\ub294 \uacc4\uce35\ncontent_layers_default = ['conv_4']\nstyle_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n\ndef get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n                               style_img, content_img,\n                               content_layers=content_layers_default,\n                               style_layers=style_layers_default):\n    # \ubaa8\ub4c8 \uc815\uaddc\ud654\n    normalization = Normalization(normalization_mean, normalization_std).to(device)\n\n    # Content / Style \uc190\uc2e4\uc774 \ubc18\ubcf5\uc801\uc73c\ub85c \uc811\uadfc\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\uae30 \uc704\ud574\n    content_losses = []\n    style_losses = []\n\n    # cnn\uc774 nn.Sequential\uc774\ub77c\uace0 \uac00\uc815\ud558\uace0,\n    # \uc21c\ucc28\uc801\uc73c\ub85c \ud65c\uc131\ud654\ub418\uc5b4\uc57c \ud558\ub294 \ubaa8\ub4c8\uc5d0 \uc0c8\ub85c\uc6b4 nn.Sequential\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\n    model = nn.Sequential(normalization)\n\n    i = 0  # conv\ub97c \ubcfc \ub54c\ub9c8\ub2e4 \uc99d\uac00\n    for layer in cnn.children():\n        if isinstance(layer, nn.Conv2d):\n            i += 1\n            name = 'conv_{}'.format(i)\n        elif isinstance(layer, nn.ReLU):\n            name = 'relu_{}'.format(i)\n            # in-place \ubc84\uc804\uc740 \uc544\ub798\uc5d0 \uc0bd\uc785\ud55c Content \uc190\uc2e4\uacfc Style \uc190\uc2e4\uc640 \uc798 \uc5b4\uc6b8\ub9ac\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n            # \uadf8\ub798\uc11c \uc5ec\uae30\uc11c\ub294 out-of-place\ub85c \ub300\uccb4\ud569\ub2c8\ub2e4.\n            layer = nn.ReLU(inplace=False)\n        elif isinstance(layer, nn.MaxPool2d):\n            name = 'pool_{}'.format(i)\n        elif isinstance(layer, nn.BatchNorm2d):\n            name = 'bn_{}'.format(i)\n        else:\n            raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n\n        model.add_module(name, layer)\n\n        if name in content_layers:\n            # Content \uc190\uc2e4 \ucd94\uac00\n            target = model(content_img).detach()\n            content_loss = ContentLoss(target)\n            model.add_module(\"content_loss_{}\".format(i), content_loss)\n            content_losses.append(content_loss)\n\n        if name in style_layers:\n            # Style \uc190\uc2e4 \ucd94\uac00\n            target_feature = model(style_img).detach()\n            style_loss = StyleLoss(target_feature)\n            model.add_module(\"style_loss_{}\".format(i), style_loss)\n            style_losses.append(style_loss)\n\n    # \uc774\uc81c \ub9c8\uc9c0\ub9c9 Content \ubc0f Style \uc190\uc2e4 \ub4a4\uc5d0 \uacc4\uce35\uc744 \uc798\ub77c\ub0c5\ub2c8\ub2e4.\n    for i in range(len(model) - 1, -1, -1):\n        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n            break\n\n    model = model[:(i + 1)]\n\n    return model, style_losses, content_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub2e4\uc74c\uc73c\ub85c \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4. Content \uc774\ubbf8\uc9c0 \uc0ac\ubcf8\uc774\ub098 \ubc31\uc0c9 \uc7a1\uc74c\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_img = content_img.clone()\n# \ub9cc\uc57d \ud654\uc774\ud2b8 \ub178\uc774\uc988(white noise)\uc744 \uc0ac\uc6a9\ud558\ub824\uba74 \uc544\ub798 \uc8fc\uc11d\uc744 \uc81c\uac70\ud558\uc138\uc694\n# input_img = torch.randn(content_img.data.size(), device=device)\n\n# \uadf8\ub9bc\uc5d0 \uc6d0\ubcf8 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.\nplt.figure()\nimshow(input_img, title='Input Image')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uacbd\uc0ac \ud558\uac15\ubc95\n----------------\n\n\n\uc54c\uace0\ub9ac\uc998 \uc791\uc131\uc790\uc778 Leon Gatys\uac00\n`\uc5ec\uae30 <https://discuss.pytorch.org/t/pytorch-tutorial-for-neural-transfert-of-artistic-style/336/20?u=alexis-jacq>`__ \uc5d0\uc11c \uc81c\uc548\ud588\ub358 \uac83\ucc98\ub7fc,\nL-BFGS \uc54c\uace0\ub9ac\uc998\uc744 \uc0ac\uc6a9\ud558\uc5ec \uacbd\uc0ac \ud558\uac15\ubc95\uc744 \uc218\ud589\ud569\ub2c8\ub2e4.\n\ub124\ud2b8\uc6cc\ud06c \ud6c8\ub828\uacfc \ub2e4\ub974\uac8c Content / Style \uc190\uc2e4\uc744 \ucd5c\uc18c\ud654\ud558\uae30 \uc704\ud574 \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ud6c8\ub828\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.\n\ud30c\uc774\ud1a0\uce58 L-BFGS optimizer ``optim.LBFGS`` \ub97c \ub9cc\ub4e4\uace0 \ucd5c\uc801\ud654 \ud560 \ud150\uc11c\ub85c \uc774\ubbf8\uc9c0\ub97c \uc804\ub2ec\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_input_optimizer(input_img):\n    # \uc785\ub825\uc774 \uae30\uc6b8\uae30\uac00 \ud544\uc694\ud55c \ub9e4\uac1c \ubcc0\uc218\uc784\uc744 \ud45c\uc2dc\ud558\ub294 \uc904\n    optimizer = optim.LBFGS([input_img])\n    return optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub9c8\uc9c0\ub9c9\uc73c\ub85c \ub274\ub7f4 \ubcc0\ud658(neural transfer)\uc744 \uc218\ud589\ud558\ub294 \ud568\uc218\ub97c \uc815\uc758\ud574\uc57c \ud569\ub2c8\ub2e4.\n\ub124\ud2b8\uc6cc\ud06c\uc758 \uac01 \ubc18\ubcf5 \ub3d9\uc548, \uc5c5\ub370\uc774\ud2b8\ub41c \uc785\ub825\uc774 \uc8fc\uc5b4\uc9c0\uace0 \uc0c8\ub85c\uc6b4 \uc190\uc2e4\uc744 \uacc4\uc0b0\ud569\ub2c8\ub2e4.\n\uac01 \uc190\uc2e4 \ubaa8\ub4c8(Loss module)\uc758 ``backward`` \uba54\uc18c\ub4dc\ub97c \uc2e4\ud589\ud558\uc5ec \uae30\uc6b8\uae30\ub97c \ub3d9\uc801\uc73c\ub85c \uacc4\uc0b0\ud569\ub2c8\ub2e4.\noptimizer\ub294 \ubaa8\ub4c8\uc744 \uc7ac\ud3c9\uac00\ud558\uace0 \uc190\uc2e4\uc744 \ubc18\ud658\ud558\ub294 \u201cclosure\u201d \ud568\uc218\uac00 \ud544\uc694\ud569\ub2c8\ub2e4.\n\n\uc5ec\uc804\ud788 \ud574\uacb0\ud574\uc57c \ud560 \ub9c8\uc9c0\ub9c9 \uc81c\uc57d\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\ub124\ud2b8\uc6cc\ud06c\uac00 \uc774\ubbf8\uc9c0\uc5d0 \ub300\ud574 0 ~ 1 \ud150\uc11c \ubc94\uc704\ub97c \ucd08\uacfc\ud558\ub294 \uac12\uc73c\ub85c \ucd5c\uc801\ud654\ud558\ub824 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ub124\ud2b8\uc6cc\ud06c\uac00 \uc2e4\ud589\ub420 \ub54c\ub9c8\ub2e4 \uc785\ub825\uac12\uc744 0\uc5d0\uc11c 1\uc0ac\uc774\ub85c \uc218\uc815\ud558\uc5ec \ubb38\uc81c\ub97c \ud574\uacb0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_style_transfer(cnn, normalization_mean, normalization_std,\n                       content_img, style_img, input_img, num_steps=300,\n                       style_weight=1000000, content_weight=1):\n    \"\"\"Run the style transfer.\"\"\"\n    print('Building the style transfer model..')\n    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n        normalization_mean, normalization_std, style_img, content_img)\n\n    # \ubaa8\ub378\uc758 \ub9e4\uac1c\ubcc0\uc218\ub97c \uc81c\uc678\ud55c \uc785\ub825\uc744 \ucd5c\uc801\ud654\ud574\uc57c \ud558\ubbc0\ub85c\n    # \uc774\uc5d0 \ub9de\ucdb0\uc11c requires_grad \uac12\uc744 \uac31\uc2e0\ud569\ub2c8\ub2e4.\n    input_img.requires_grad_(True)\n    model.requires_grad_(False)\n\n    optimizer = get_input_optimizer(input_img)\n\n    print('Optimizing..')\n    run = [0]\n    while run[0] <= num_steps:\n\n        def closure():\n            # \uc5c5\ub370\uc774\ud2b8 \ub41c \uc785\ub825 \uc774\ubbf8\uc9c0\uc758 \uac12\uc744 \uc218\uc815\n            with torch.no_grad():\n                input_img.clamp_(0, 1)\n\n            optimizer.zero_grad()\n            model(input_img)\n            style_score = 0\n            content_score = 0\n\n            for sl in style_losses:\n                style_score += sl.loss\n            for cl in content_losses:\n                content_score += cl.loss\n\n            style_score *= style_weight\n            content_score *= content_weight\n\n            loss = style_score + content_score\n            loss.backward()\n\n            run[0] += 1\n            if run[0] % 50 == 0:\n                print(\"run {}:\".format(run))\n                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n                    style_score.item(), content_score.item()))\n                print()\n\n            return style_score + content_score\n\n        optimizer.step(closure)\n\n    # \ub9c8\uc9c0\ub9c9 \uc218\uc815...\n    with torch.no_grad():\n        input_img.clamp_(0, 1)\n\n    return input_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub9c8\uc9c0\ub9c9\uc73c\ub85c, \uc54c\uace0\ub9ac\uc998\uc744 \uc2e4\ud589\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n                            content_img, style_img, input_img)\n\nplt.figure()\nimshow(output, title='Output Image')\n\n# sphinx_gallery_thumbnail_number = 4\nplt.ioff()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}