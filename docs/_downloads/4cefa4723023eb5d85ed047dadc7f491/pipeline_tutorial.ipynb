{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\ud30c\uc774\ud504\ub77c\uc778 \ubcd1\ub82c\ud654\ub85c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378 \ud559\uc2b5\uc2dc\ud0a4\uae30\n==============================================\n\n**Author**: `Pritam Damania <https://github.com/pritamdamania87>`_\n  **\ubc88\uc5ed**: `\ubc31\uc120\ud76c <https://github.com/spongebob03>`_\n\n\uc774 \ud29c\ud1a0\ub9ac\uc5bc\uc740 \ud30c\uc774\ud504\ub77c\uc778(pipeline) \ubcd1\ub82c\ud654(parallelism)\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5ec\ub7ec GPU\uc5d0 \uac78\uce5c \uac70\ub300\ud55c \ud2b8\ub79c\uc2a4\ud3ec\uba38(transformer)\n\ubaa8\ub378\uc744 \uc5b4\ub5bb\uac8c \ud559\uc2b5\uc2dc\ud0a4\ub294\uc9c0 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4. `NN.TRANSFORMER \uc640 TORCHTEXT \ub85c \uc2dc\ud000\uc2a4-\ud22c-\uc2dc\ud000\uc2a4(SEQUENCE-TO-SEQUENCE) \ubaa8\ub378\ub9c1\ud558\uae30 <https://tutorials.pytorch.kr/beginner/transformer_tutorial.html>`__ \ud29c\ud1a0\ub9ac\uc5bc\uc758\n\ud655\uc7a5\ud310\uc774\uba70 \ud30c\uc774\ud504\ub77c\uc778 \ubcd1\ub82c\ud654\uac00 \uc5b4\ub5bb\uac8c \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378 \ud559\uc2b5\uc5d0 \uc4f0\uc774\ub294\uc9c0 \uc99d\uba85\ud558\uae30 \uc704\ud574 \uc774\uc804 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\uc758\n\ubaa8\ub378 \uaddc\ubaa8\ub97c \uc99d\uac00\uc2dc\ucf30\uc2b5\ub2c8\ub2e4.\n\n\uc120\uc218\uacfc\ubaa9(Prerequisites):\n\n    * `Pipeline Parallelism <https://pytorch.org/docs/stable/pipeline.html>`__\n    * `NN.TRANSFORMER \uc640 TORCHTEXT \ub85c \uc2dc\ud000\uc2a4-\ud22c-\uc2dc\ud000\uc2a4(SEQUENCE-TO-SEQUENCE) \ubaa8\ub378\ub9c1\ud558\uae30 <https://tutorials.pytorch.kr/beginner/transformer_tutorial.html>`__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \uc815\uc758\ud558\uae30\n-------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc774\ubc88 \ud29c\ud1a0\ub9ac\uc5bc\uc5d0\uc11c\ub294, \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc744 \ub450 \uac1c\uc758 GPU\uc5d0 \uac78\uccd0\uc11c \ub098\ub204\uace0 \ud30c\uc774\ud504\ub77c\uc778 \ubcd1\ub82c\ud654\ub85c \ud559\uc2b5\uc2dc\ucf1c \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\ubaa8\ub378\uc740 \ubc14\ub85c `NN.TRANSFORMER \uc640 TORCHTEXT \ub85c \uc2dc\ud000\uc2a4-\ud22c-\uc2dc\ud000\uc2a4(SEQUENCE-TO-SEQUENCE) \ubaa8\ub378\ub9c1\ud558\uae30\n<https://tutorials.pytorch.kr/beginner/transformer_tutorial.html>`__ \ud29c\ud1a0\ub9ac\uc5bc\uacfc\n\ub611\uac19\uc740 \ubaa8\ub378\uc774\uc9c0\ub9cc \ub450 \ub2e8\uacc4\ub85c \ub098\ub269\ub2c8\ub2e4. \ub300\ubd80\ubd84 \ud30c\ub77c\ubbf8\ud130(parameter)\ub4e4\uc740\n`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>`__ \uacc4\uce35(layer)\uc5d0 \ud3ec\ud568\ub429\ub2c8\ub2e4.\n`nn.TransformerEncoder <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html>`__ \ub294\n`nn.TransformerEncoderLayer <https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html>`__ \uc758 ``nlayers`` \ub85c \uad6c\uc131\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4.\n\uacb0\uacfc\uc801\uc73c\ub85c, \uc6b0\ub9ac\ub294 ``nn.TransformerEncoder`` \uc5d0 \uc911\uc810\uc744 \ub450\uace0 \uc788\uc73c\uba70,\n``nn.TransformerEncoderLayer`` \uc758 \uc808\ubc18\uc740 \ud55c GPU\uc5d0 \ub450\uace0\n\ub098\uba38\uc9c0 \uc808\ubc18\uc740 \ub2e4\ub978 GPU\uc5d0 \uc788\ub3c4\ub85d \ubaa8\ub378\uc744 \ubd84\ud560\ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574\uc11c ``Encoder`` \uc640\n``Decoder`` \uc139\uc158\uc744 \ubd84\ub9ac\ub41c \ubaa8\ub4c8\ub85c \ube7c\ub0b8 \ub2e4\uc74c, \uc6d0\ubcf8 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub4c8\uc744\n\ub098\ud0c0\ub0b4\ub294 nn.Sequential\uc744 \ube4c\ub4dc \ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sys\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport tempfile\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\n\nif sys.platform == 'win32':\n    print('Windows platform is not supported for pipeline parallelism')\n    sys.exit(0)\nif torch.cuda.device_count() < 2:\n    print('Need at least two GPU devices for this tutorial')\n    sys.exit(0)\n\nclass Encoder(nn.Module):\n    def __init__(self, ntoken, ninp, dropout=0.5):\n        super(Encoder, self).__init__()\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        self.encoder = nn.Embedding(ntoken, ninp)\n        self.ninp = ninp\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src):\n        # \uc778\ucf54\ub354\ub85c (S, N) \ud3ec\ub9f7\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n        src = src.t()\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        return self.pos_encoder(src)\n\nclass Decoder(nn.Module):\n    def __init__(self, ntoken, ninp):\n        super(Decoder, self).__init__()\n        self.decoder = nn.Linear(ninp, ntoken)\n        self.init_weights()\n\n    def init_weights(self):\n        initrange = 0.1\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, inp):\n        # \ud30c\uc774\ud504\ub77c\uc778 \uacb0\uacfc\ubb3c\uc744 \uc704\ud574 \uba3c\uc800 \ubc30\uce58 \ucc28\uc6d0 \ud544\uc694\ud569\ub2c8\ub2e4.\n        return self.decoder(inp).permute(1, 0, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``PositionalEncoding``\u00a0\ubaa8\ub4c8\uc740 \uc2dc\ud000\uc2a4 \uc548\uc5d0\uc11c \ud1a0\ud070\uc758 \uc0c1\ub300\uc801\uc778 \ub610\ub294 \uc808\ub300\uc801\uc778 \ud3ec\uc9c0\uc158\uc5d0 \ub300\ud55c \uc815\ubcf4\ub97c \uc8fc\uc785\ud569\ub2c8\ub2e4.\n\ud3ec\uc9c0\uc154\ub110 \uc778\ucf54\ub529\uc740 \uc784\ubca0\ub529\uacfc \ud569\uce60 \uc218 \uc788\ub3c4\ub85d \ub611\uac19\uc740 \ucc28\uc6d0\uc744 \uac00\uc9d1\ub2c8\ub2e4. \uc5ec\uae30\uc11c\n\ub2e4\ub978 \uc8fc\uae30(frequency)\uc758\u00a0``sine``\u00a0\uacfc\u00a0``cosine``\u00a0\ud568\uc218\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ub370\uc774\ud130 \ub85c\ub4dc\ud558\uace0 \ubc30\uce58 \ub9cc\ub4e4\uae30\n---------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud559\uc2b5 \ud504\ub85c\uc138\uc2a4\ub294 ``torchtext`` \uc758 Wikitext-2 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\ntorchtext \ub370\uc774\ud130\uc14b\uc5d0 \uc811\uadfc\ud558\uae30 \uc804\uc5d0, https://github.com/pytorch/data \uc744 \ucc38\uace0\ud558\uc5ec torchdata\ub97c \uc124\uce58\ud558\uc2dc\uae30 \ubc14\ub78d\ub2c8\ub2e4.\n\n\ub2e8\uc5b4 \uc624\ube0c\uc81d\ud2b8\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ub9cc\ub4e4\uc5b4\uc9c0\uace0, \ud1a0\ud070\uc744 \ud150\uc11c(tensor)\ub85c \uc218\uce58\ud654\ud558\ub294\ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n\uc2dc\ud000\uc2a4 \ub370\uc774\ud130\ub85c\ubd80\ud130 \uc2dc\uc791\ud558\uc5ec, ``batchify()`` \ud568\uc218\ub294 \ub370\uc774\ud130\uc14b\uc744 \uc5f4(column)\ub4e4\ub85c \uc815\ub9ac\ud558\uace0,\n``batch_size`` \uc0ac\uc774\uc988\uc758 \ubc30\uce58\ub4e4\ub85c \ub098\ub208 \ud6c4\uc5d0 \ub0a8\uc740 \ubaa8\ub4e0 \ud1a0\ud070\uc744 \ubc84\ub9bd\ub2c8\ub2e4.\n\uc608\ub97c \ub4e4\uc5b4, \uc54c\ud30c\ubcb3\uc744 \uc2dc\ud000\uc2a4(\ucd1d \uae38\uc774 26)\ub85c \uc0dd\uac01\ud558\uace0 \ubc30\uce58 \uc0ac\uc774\uc988\ub97c 4\ub77c\uace0 \ud55c\ub2e4\uba74,\n\uc54c\ud30c\ubcb3\uc744 \uae38\uc774\uac00 6\uc778 4\uac1c\uc758 \uc2dc\ud000\uc2a4\ub85c \ub098\ub20c \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n\\begin{align}\\begin{bmatrix}\n  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n  \\end{bmatrix}\n  \\Rightarrow\n  \\begin{bmatrix}\n  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n  \\end{bmatrix}\\end{align}\n\n\uc774 \uc5f4\ub4e4\uc740 \ubaa8\ub378\uc5d0 \uc758\ud574\uc11c \ub3c5\ub9bd\uc801\uc73c\ub85c \ucde8\uae09\ub418\uba70, \uc774\ub294\n``G``\u00a0\uc640\u00a0``F``\u00a0\uc758 \uc758\uc874\uc131\uc774 \ud559\uc2b5\ub420 \uc218 \uc5c6\ub2e4\ub294 \uac83\uc744 \uc758\ubbf8\ud558\uc9c0\ub9cc, \ub354 \ud6a8\uc728\uc801\uc778\n\ubc30\uce58 \ud504\ub85c\uc138\uc2f1(batch processing)\uc744 \ud5c8\uc6a9\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torchtext.datasets import WikiText2\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntrain_iter = WikiText2(split='train')\ntokenizer = get_tokenizer('basic_english')\nvocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=[\"<unk>\"])\nvocab.set_default_index(vocab[\"<unk>\"])\n\ndef data_process(raw_text_iter):\n  data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n\ntrain_iter, val_iter, test_iter = WikiText2()\ntrain_data = data_process(train_iter)\nval_data = data_process(val_iter)\ntest_data = data_process(test_iter)\n\ndevice = torch.device(\"cuda\")\n\ndef batchify(data, bsz):\n    # \ub370\uc774\ud130\uc14b\uc744 bsz \ud30c\ud2b8\ub4e4\ub85c \ub098\ub215\ub2c8\ub2e4.\n    nbatch = data.size(0) // bsz\n    # \uae54\ub054\ud558\uac8c \ub098\ub204\uc5b4 \ub5a8\uc5b4\uc9c0\uc9c0 \uc54a\ub294 \ucd94\uac00\uc801\uc778 \ubd80\ubd84(\ub098\uba38\uc9c0)\uc740 \uc798\ub77c\ub0c5\ub2c8\ub2e4.\n    data = data.narrow(0, 0, nbatch * bsz)\n    # \ub370\uc774\ud130\ub97c bsz \ubc30\uce58\ub4e4\ub85c \ub3d9\uc77c\ud558\uac8c \ub098\ub215\ub2c8\ub2e4.\n    data = data.view(bsz, -1).t().contiguous()\n    return data.to(device)\n\nbatch_size = 20\neval_batch_size = 10\ntrain_data = batchify(train_data, batch_size)\nval_data = batchify(val_data, eval_batch_size)\ntest_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc785\ub825\uacfc \ud0c0\uac9f \uc2dc\ud000\uc2a4\ub97c \uc0dd\uc131\ud558\uae30 \uc704\ud55c \ud568\uc218\ub4e4\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``get_batch()`` \ud568\uc218\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc744 \uc704\ud55c \uc785\ub825\uacfc \ud0c0\uac9f \uc2dc\ud000\uc2a4\ub97c\n\uc0dd\uc131\ud569\ub2c8\ub2e4. \uc774 \ud568\uc218\ub294 \uc18c\uc2a4 \ub370\uc774\ud130\ub97c ``bptt`` \uae38\uc774\ub97c \uac00\uc9c4 \ub369\uc5b4\ub9ac\ub85c \uc138\ubd84\ud654\ud569\ub2c8\ub2e4.\n\uc5b8\uc5b4 \ubaa8\ub378\ub9c1 \uacfc\uc81c\ub97c \uc704\ud574\uc11c, \ubaa8\ub378\uc740 \ub2e4\uc74c \ub2e8\uc5b4\uc778 ``Target`` \uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uc5d0\ub97c \ub4e4\uc5b4 ``bptt`` \uc758 \uac12\uc774 2\ub77c\uba74,\n``i`` = 0 \uc77c \ub54c \ub2e4\uc74c\uc758 2 \uac1c \ubcc0\uc218(Variable)\ub97c \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4:\n\n![](../_static/img/transformer_input_target.png)\n\n\n\ubcc0\uc218 \ub369\uc5b4\ub9ac\ub294 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378\uc758\u00a0``S``\u00a0\ucc28\uc6d0\uacfc \uc77c\uce58\ud558\ub294 0 \ucc28\uc6d0\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4.\n\ubc30\uce58 \ucc28\uc6d0\u00a0``N``\u00a0\uc740 1 \ucc28\uc6d0\uc5d0 \ud574\ub2f9\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bptt = 25\ndef get_batch(source, i):\n    seq_len = min(bptt, len(source) - 1 - i)\n    data = source[i:i+seq_len]\n    target = source[i+1:i+1+seq_len].view(-1)\n    # \ud30c\uc774\ud504\ub77c\uc778 \ubcd1\ub82c\ud654\ub97c \uc704\ud574 \uba3c\uc800 \ubc30\uce58 \ucc28\uc6d0\uc774 \ud544\uc694\ud569\ub2c8\ub2e4.\n    return data.t(), target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \uaddc\ubaa8\uc640 \ud30c\uc774\ud504 \ucd08\uae30\ud654\n-------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud30c\uc774\ud504\ub77c\uc778 \ubcd1\ub82c\ud654\ub97c \ud65c\uc6a9\ud55c \ub300\ud615 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ubaa8\ub378 \ud559\uc2b5\uc744 \uc99d\uba85\ud558\uae30 \uc704\ud574\uc11c\n\ud2b8\ub79c\uc2a4\ud3ec\uba38 \uacc4\uce35 \uaddc\ubaa8\ub97c \uc801\uc808\ud788 \ud655\uc7a5\uc2dc\ud0b5\ub2c8\ub2e4. 4096\ucc28\uc6d0\uc758 \uc784\ubca0\ub529 \ubca1\ud130, 4096\uc758 \uc740\ub2c9 \uc0ac\uc774\uc988,\n16\uac1c\uc758 \uc5b4\ud150\uc158 \ud5e4\ub4dc(attention head)\uc640 \ucd1d 12 \uac1c\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \uacc4\uce35\n(``nn.TransformerEncoderLayer``)\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub294 \ucd5c\ub300\n**1.4\uc5b5** \uac1c\uc758 \ud30c\ub77c\ubbf8\ud130\ub97c \uac16\ub294 \ubaa8\ub378\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.\n\nPipe\ub294 `RRef <https://pytorch.org/docs/stable/rpc.html#rref>`__ \ub97c \ud1b5\ud574\n`RPC \ud504\ub808\uc784\uc6cc\ud06c <https://pytorch.org/docs/stable/rpc.html>`__ \uc5d0 \uc758\uc874\ud558\ub294\ub370\n\uc774\ub294 \ud5a5\ud6c4 \ud638\uc2a4\ud2b8 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uad50\ucc28 \ud655\uc7a5\ud560 \uc218 \uc788\ub3c4\ub85d \ud558\uae30 \ub54c\ubb38\uc5d0\nRPC \ud504\ub808\uc784\uc6cc\ud06c\ub97c \ucd08\uae30\ud654\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub54c RPC \ud504\ub808\uc784\uc6cc\ud06c\ub294 \uc624\uc9c1 \ud558\ub098\uc758 \ud558\ub098\uc758 worker\ub85c \ucd08\uae30\ud654\ub97c \ud574\uc57c \ud558\ub294\ub370,\n\uc5ec\ub7ec GPU\ub97c \ub2e4\ub8e8\uae30 \uc704\ud574 \ud504\ub85c\uc138\uc2a4 \ud558\ub098\ub9cc \uc0ac\uc6a9\ud558\uace0 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.\n\n\uadf8\ub7f0 \ub2e4\uc74c \ud30c\uc774\ud504\ub77c\uc778\uc740 \ud55c GPU\uc5d0 8\uac1c\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38\uc640\n\ub2e4\ub978 GPU\uc5d0 8\uac1c\uc758 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ub808\uc774\uc5b4\ub85c \ucd08\uae30\ud654\ub429\ub2c8\ub2e4.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>\ud6a8\uc728\uc131\uc744 \uc704\ud574 ``Pipe`` \uc5d0 \uc804\ub2ec\ub41c ``nn.Sequential`` \uc774\n   \uc624\uc9c1 \ub450 \uac1c\uc758 \uc694\uc18c(2\uac1c\uc758 GPU)\ub85c\ub9cc \uad6c\uc131\ub418\ub3c4\ub85d \ud569\ub2c8\ub2e4. \uc774\ub807\uac8c \ud558\uba74\n   Pipe\uac00 \ub450 \uac1c\uc758 \ud30c\ud2f0\uc158\uc5d0\uc11c\ub9cc \uc791\ub3d9\ud558\uace0\n   \ud30c\ud2f0\uc158 \uac04 \uc624\ubc84\ud5e4\ub4dc\ub97c \ud53c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ntokens = len(vocab) # \ub2e8\uc5b4 \uc0ac\uc804(\uc5b4\ud718\uc9d1)\uc758 \ud06c\uae30\nemsize = 4096 # \uc784\ubca0\ub529 \ucc28\uc6d0\nnhid = 4096 # nn.TransformerEncoder \uc5d0\uc11c \uc21c\uc804\ud30c(feedforward) \uc2e0\uacbd\ub9dd \ubaa8\ub378\uc758 \ucc28\uc6d0\nnlayers = 12 # nn.TransformerEncoder \ub0b4\ubd80\uc758 nn.TransformerEncoderLayer \uac1c\uc218\nnhead = 16 # multiheadattention \ubaa8\ub378\uc758 \ud5e4\ub4dc \uac1c\uc218\ndropout = 0.2 # dropout \uac12\n\nfrom torch.distributed import rpc\ntmpfile = tempfile.NamedTemporaryFile()\nrpc.init_rpc(\n    name=\"worker\",\n    rank=0,\n    world_size=1,\n    rpc_backend_options=rpc.TensorPipeRpcBackendOptions(\n        init_method=\"file://{}\".format(tmpfile.name),\n        # _transports\uc640 _channels\ub97c \uc9c0\uc815\ud558\ub294 \uac83\uc774 \ud574\uacb0 \ubc29\ubc95\uc774\uba70\n        # PyTorch \ubc84\uc804 >= 1.8.1 \uc5d0\uc11c\ub294 _transports\uc640 _channels\ub97c\n        # \uc9c0\uc815\ud558\uc9c0 \uc54a\uc544\ub3c4 \ub429\ub2c8\ub2e4.\n        _transports=[\"ibv\", \"uv\"],\n        _channels=[\"cuda_ipc\", \"cuda_basic\"],\n    )\n)\n\nnum_gpus = 2\npartition_len = ((nlayers - 1) // num_gpus) + 1\n\n# \ucc98\uc74c\uc5d0 \uc778\ucf54\ub354\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.\ntmp_list = [Encoder(ntokens, emsize, dropout).cuda(0)]\nmodule_list = []\n\n# \ud544\uc694\ud55c \ubaa8\ub4e0 \ud2b8\ub79c\uc2a4\ud3ec\uba38 \ube14\ub85d\ub4e4\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4.\nfor i in range(nlayers):\n    transformer_block = TransformerEncoderLayer(emsize, nhead, nhid, dropout)\n    if i != 0 and i % (partition_len) == 0:\n        module_list.append(nn.Sequential(*tmp_list))\n        tmp_list = []\n    device = i // (partition_len)\n    tmp_list.append(transformer_block.to(device))\n\n# \ub9c8\uc9c0\ub9c9\uc5d0 \ub514\ucf54\ub354\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.\ntmp_list.append(Decoder(ntokens, emsize).cuda(num_gpus - 1))\nmodule_list.append(nn.Sequential(*tmp_list))\n\nfrom torch.distributed.pipeline.sync import Pipe\n\n# \ud30c\uc774\ud504\ub77c\uc778\uc744 \ube4c\ub4dc\ud569\ub2c8\ub2e4.\nchunks = 8\nmodel = Pipe(torch.nn.Sequential(*module_list), chunks = chunks)\n\n\ndef get_total_params(module: torch.nn.Module):\n    total_params = 0\n    for param in module.parameters():\n        total_params += param.numel()\n    return total_params\n\nprint ('Total parameters in model: {:,}'.format(get_total_params(model)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ubaa8\ub378 \uc2e4\ud589\ud558\uae30\n-------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc190\uc2e4(loss)\uc744 \ucd94\uc801\ud558\uae30 \uc704\ud574 `CrossEntropyLoss <https://pytorch.org/docs/master/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__ \uac00\n\uc801\uc6a9\ub418\uba70, \uc635\ud2f0\ub9c8\uc774\uc800(optimizer)\ub85c\uc11c `SGD <https://pytorch.org/docs/master/optim.html?highlight=sgd#torch.optim.SGD>`__\n\ub294 \ud655\ub960\uc801 \uacbd\uc0ac\ud558\uac15\ubc95(stochastic gradient descent method)\uc744 \uad6c\ud604\ud569\ub2c8\ub2e4. \ucd08\uae30\n\ud559\uc2b5\ub960(learning rate)\uc740 5.0\ub85c \uc124\uc815\ub429\ub2c8\ub2e4. `StepLR <https://pytorch.org/docs/master/optim.html?highlight=steplr#torch.optim.lr_scheduler.StepLR>`__ \ub294\n\uc5d0\ud3ed(epoch)\uc5d0 \ub530\ub77c\uc11c \ud559\uc2b5\ub960\uc744 \uc870\uc808\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \ud559\uc2b5\ud558\ub294 \ub3d9\uc548,\n\uae30\uc6b8\uae30 \ud3ed\ubc1c(gradient exploding)\uc744 \ubc29\uc9c0\ud558\uae30 \uc704\ud574 \ubaa8\ub4e0 \uae30\uc6b8\uae30\ub97c \ud568\uaed8 \uc870\uc815(scale)\ud558\ub294 \ud568\uc218\n`nn.utils.clip_grad_norm\\_ <https://pytorch.org/docs/master/nn.html?highlight=nn%20utils%20clip_grad_norm#torch.nn.utils.clip_grad_norm_>`__\n\uc744 \uc774\uc6a9\ud569\ub2c8\ub2e4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\nlr = 5.0 # \ud559\uc2b5\ub960\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n\nimport time\ndef train():\n    model.train() # \ud6c8\ub828 \ubaa8\ub4dc\ub85c \uc804\ud658\n    total_loss = 0.\n    start_time = time.time()\n    ntokens = len(vocab)\n\n    # \uc2a4\ud06c\ub9bd\ud2b8 \uc2e4\ud589 \uc2dc\uac04\uc744 \uc9e7\uac8c \uc720\uc9c0\ud558\uae30 \uc704\ud574\uc11c 50 \ubc30\uce58\ub9cc \ud559\uc2b5\ud569\ub2c8\ub2e4.\n    nbatches = min(50 * bptt, train_data.size(0) - 1)\n\n    for batch, i in enumerate(range(0, nbatches, bptt)):\n        data, targets = get_batch(train_data, i)\n        optimizer.zero_grad()\n        # Pipe\ub294 \ub2e8\uc77c \ud638\uc2a4\ud2b8 \ub0b4\uc5d0 \uc788\uace0\n        # forward \uba54\uc11c\ub4dc\ub85c \ubc18\ud658\ub41c ``RRef`` \ud504\ub85c\uc138\uc2a4\ub294 \uc774 \ub178\ub4dc\uc5d0 \uad6d\ud55c\ub418\uc5b4 \uc788\uae30 \ub54c\ubb38\uc5d0\n        # ``RRef.local_value()`` \ub97c \ud1b5\ud574 \uac04\ub2e8\ud788 \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n        output = model(data).local_value()\n        # \ud0c0\uac9f\uc744 \ud30c\uc774\ud504\ub77c\uc778 \ucd9c\ub825\uc774 \uc788\ub294\n        # \uc7a5\uce58\ub85c \uc62e\uaca8\uc57c\ud569\ub2c8\ub2e4.\n        loss = criterion(output.view(-1, ntokens), targets.cuda(1))\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n        optimizer.step()\n\n        total_loss += loss.item()\n        log_interval = 10\n        if batch % log_interval == 0 and batch > 0:\n            cur_loss = total_loss / log_interval\n            elapsed = time.time() - start_time\n            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n                  'lr {:02.2f} | ms/batch {:5.2f} | '\n                  'loss {:5.2f} | ppl {:8.2f}'.format(\n                    epoch, batch, nbatches // bptt, scheduler.get_lr()[0],\n                    elapsed * 1000 / log_interval,\n                    cur_loss, math.exp(cur_loss)))\n            total_loss = 0\n            start_time = time.time()\n\ndef evaluate(eval_model, data_source):\n    eval_model.eval() # \ud3c9\uac00 \ubaa8\ub4dc\ub85c \uc804\ud658\n    total_loss = 0.\n    ntokens = len(vocab)\n    # \uc2a4\ud06c\ub9bd\ud2b8 \uc2e4\ud589 \uc2dc\uac04\uc744 \uc9e7\uac8c \uc720\uc9c0\ud558\uae30 \uc704\ud574 50 \ubc30\uce58\ub9cc \ud3c9\uac00\ud569\ub2c8\ub2e4.\n    nbatches = min(50 * bptt, data_source.size(0) - 1)\n    with torch.no_grad():\n        for i in range(0, nbatches, bptt):\n            data, targets = get_batch(data_source, i)\n            output = eval_model(data).local_value()\n            output_flat = output.view(-1, ntokens)\n            # \ud0c0\uac9f\uc744 \ud30c\uc774\ud504\ub77c\uc778 \ucd9c\ub825\uc774 \uc788\ub294\n            # \uc7a5\uce58\ub85c \uc62e\uaca8\uc57c\ud569\ub2c8\ub2e4.\n            total_loss += len(data) * criterion(output_flat, targets.cuda(1)).item()\n    return total_loss / (len(data_source) - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\uc5d0\ud3ed\uc744 \ubc18\ubcf5\ud569\ub2c8\ub2e4. \ub9cc\uc57d \uac80\uc99d \uc624\ucc28(validation loss)\uac00 \uc9c0\uae08\uae4c\uc9c0 \uad00\ucc30\ud55c \uac83 \uc911 \ucd5c\uc801\uc774\ub77c\uba74\n\ubaa8\ub378\uc744 \uc800\uc7a5\ud569\ub2c8\ub2e4. \uac01 \uc5d0\ud3ed \uc774\ud6c4\uc5d0 \ud559\uc2b5\ub960\uc744 \uc870\uc808\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_val_loss = float(\"inf\")\nepochs = 3 # \uc5d0\ud3ed \uc218\nbest_model = None\n\nfor epoch in range(1, epochs + 1):\n    epoch_start_time = time.time()\n    train()\n    val_loss = evaluate(model, val_data)\n    print('-' * 89)\n    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n                                     val_loss, math.exp(val_loss)))\n    print('-' * 89)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model = model\n\n    scheduler.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ubaa8\ub378 \ud3c9\uac00\ud558\uae30\n-------------------------------\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ud3c9\uac00 \ub370\uc774\ud130\uc14b\uc5d0\uc11c\uc758 \uacb0\uacfc\ub97c \ud655\uc778\ud558\uae30 \uc704\ud574 \ucd5c\uace0\uc758 \ubaa8\ub378\uc744 \uc801\uc6a9\ud569\ub2c8\ub2e4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_loss = evaluate(best_model, test_data)\nprint('=' * 89)\nprint('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n    test_loss, math.exp(test_loss)))\nprint('=' * 89)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}